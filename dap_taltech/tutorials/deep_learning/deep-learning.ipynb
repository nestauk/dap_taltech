{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the deep learning tutorial üëã!\n",
    "\n",
    "The **goals** of the tutorial are to\n",
    "\n",
    "1. Learn about the building blocks of PyTorch, from layer creation to regularisation and model training.\n",
    "2. Train feed-forward neural networks on traditional and computer vision tasks.\n",
    "3. Explore how more advanced network structures can be leveraged to perform time series analysis, and how convolutional layers impact the performance of neural networks on computer vision tasks.\n",
    "4. Load pre-trained language models from huggingface and use transfer learning to showcase NLP applications.\n",
    "\n",
    "#### Documentation\n",
    "\n",
    "- [PyTorch](https://pytorch.org)\n",
    "- [huggingface](https://huggingface.co)\n",
    "\n",
    "#### Repos\n",
    "\n",
    "- [Voight, D. (2022) - Deep Learning with pyTorch Step-by-Step](https://github.com/dvgodoy/PyTorchStepByStep)\n",
    "\n",
    "#### Books\n",
    "\n",
    "- [Goodfellow et al (2016) - The Deep Learning Book](https://www.deeplearningbook.org)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from toolz import pipe\n",
    "from typing import Dict, Sequence\n",
    "from PIL import Image\n",
    "\n",
    "from dap_taltech.utils.data_getters import DataGetter\n",
    "from dap_taltech import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.data_transformers.enable('default', max_rows=None)\n",
    "getter = DataGetter(local=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch üöÄ\n",
    "\n",
    "In the universe of deep learning, PyTorch shines like a bright star. An open-source library originally developed by the Facebook AI Research lab, PyTorch provides a platform for all your computation needs, with a particular focus on deep neural networks.\n",
    "\n",
    "At the core of PyTorch, we find tensors - multi-dimensional arrays similar to NumPy ndarrays but with superpowers. These tensors can be used on a GPU, enabling a significant speedup in computations, a crucial aspect for deep learning tasks which has spurred the surge in deep learning capability in recent times.\n",
    "\n",
    "In Pytorch, tensors can be declared simply in one of several ways:\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/2000/1*_D5ZvufDS38WkhK9rK32hQ.jpeg\" width=\"800\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What sets PyTorch apart is its dynamic computation graph, offering flexibility in building complex architectural models. It also provides excellent support for gradient-based optimisation through automatic differentiation, a backbone of backpropagation in neural networks.\n",
    "\n",
    "Lastly, it's worth mentioning PyTorch's rich ecosystem that includes TorchText, TorchVision, and TorchAudio for text, image, and audio processing, respectively. Together, they make PyTorch a versatile tool, suited to solve a broad range of AI tasks.\n",
    "\n",
    "In this section we're going to first introduce and then build a standard PyTorch workflow, which largely looks as follows:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01_a_pytorch_workflow.png\" width=900 alt=\"a pytorch workflow flowchat\"/>\n",
    "\n",
    "So strap in and get ready to launch into the deep learning cosmos with PyTorch! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check PyTorch version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the code below, we are configuring our computing setup to be device agnostic. This means, if we are lucky enough to have a GPU, we will leverage its processing power and adequacy to handle tensors. In its absence, we shall have to rely on the good old CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A primer on Tensors\n",
    "\n",
    "Tensors form the essential building blocks in the realm of deep learning libraries, including PyTorch. A tensor, in the simplest terms, is a generalization of vectors and matrices to potentially higher dimensions, with the ability to represent n-dimensional arrays of data. \n",
    "\n",
    "In PyTorch, these tensors behave similarly to the ndarray objects in NumPy, but with an added advantage of being usable on a GPU, thereby facilitating faster computational operations. Further, these tensors also keep track of the computational graph and gradients, acting as key players in the automatic differentiation system of PyTorch, which is central to training neural networks. \n",
    "\n",
    "So, understanding tensors essentially becomes the stepping stone into the world of deep learning. Let's do just that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An empty tensor\n",
    "tensor = torch.empty(6,4, dtype=torch.float)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the elements are not necessarily zero; rather, they are the values in RAM / VRAM memory at the location where the tensor was created. The reason for this is efficiency: if you know you will be writing to all the elements of the tensor, then there is no need to spend time initialising them, so torch.empty() doesn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can always retrieve the shape of the tensor x, providing the dimensions of the tensor in a convenient tuple format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, you can conveniently convert tensors into numpy arrays, enabling easy integration with existing Python workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse is also true: you can use a numpy array and transform it to a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array([[1,5],[2,3]])\n",
    "torch.from_numpy(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Operations üõ†Ô∏è\n",
    "\n",
    "Creating tensors and understanding their basic properties is important, but the real magic of tensors comes into play when we start performing operations on them.\n",
    "\n",
    "##### Random numbers similar to numpy üé≤\n",
    "\n",
    "We can easily create a tensor filled with random numbers from a uniform distribution on the interval [0, 1), similar to how we do it in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(6, 4)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construct a matrix filled with zeros and of dtype long üéØ\n",
    "We can also create tensors filled with zeros (or any other number), specifying their type at creation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.zeros(6, 4, dtype=torch.long)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.ones(2, 2, dtype=torch.long)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construct a tensor directly from data üìö\n",
    "Directly constructing a tensor from a list of numbers is also an option. Remember, the tensor will copy the data, not reference it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([3, 2.5])\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create tensor based on existing tensor üîÑ\n",
    "In some cases, you might want to create a tensor that has the same properties as another tensor (same dtype and same device), but filled with different data. For this, PyTorch provides the new_ones and new_zeros methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tensor.new_ones(3, 3, dtype=torch.double)\n",
    "print(tensor)\n",
    "print(tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.randn_like(tensor, dtype=torch.float)\n",
    "print(tensor) \n",
    "print(tensor.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Tensor Operation üßÆ\n",
    "Performing arithmetic operations on tensors is easy and intuitive. Under the hood, these operations are highly optimised and can run in parallel on a GPU. This makes PyTorch a powerful tool for numerical computations, similar to numpy but with powerful GPU or other hardware acceleration.\n",
    "\n",
    "<img src=\"https://devblogs.nvidia.com/wp-content/uploads/2018/05/tesnor_core_diagram.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "We have a variety of basic operations such as addition(torch.add()), subtraction(torch.sub()), division(torch.div()), and multiplication(torch.mul()) that can be performed on tensors. Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "print(x + y) # Standard Python addition\n",
    "print(torch.add(x, y)) # PyTorch method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "print(x - y) # Standard Python subtraction\n",
    "print(torch.sub(x, y)) # PyTorch method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "print(x / y) # Standard Python division\n",
    "print(torch.div(x, y)) # PyTorch method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "print(x * y) # Standard Python multiplication\n",
    "print(torch.mul(x, y)) # PyTorch method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch also provides an in-place addition method. This means the addition operation is done on the tensor and the result is stored in the same tensor, without creating a new one. Remember that in PyTorch, every method that ends with an underscore performs an in-place operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.add_(x)  # adds x to y\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch tensors also support standard numpy-like indexing with all the bells and whistles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[:, 1])  # Prints the second column of tensor x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resizing Tensors\n",
    "PyTorch also provides methods to reshape tensors, similar to numpy's reshape function. The view method in PyTorch can be used to reshape a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(6, 6)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.view(36)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x.view(-1, 4)  # the size -1 is inferred from other dimensions\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! You have learned how to perform basic operations with PyTorch tensors! The power of PyTorch, however, really shines when building complex neural network models, as we'll see in a few sections. Keep going!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensors and the ability to derivate üî•\n",
    "\n",
    "One of the pivotal elements setting PyTorch apart from other deep learning frameworks is its dynamic computational graph paradigm, enabling on-the-go generation and modification of models. It's been made possible primarily due to the torch.Tensor object.\n",
    "\n",
    "![](https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/dynamic_graph.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence, a torch.Tensor wraps the functionality previously catered to by torch.autograd.Variable. In the older versions of PyTorch, Variable used to be the central class of the package and was pivotal for automatic differentiation. However, with newer versions of PyTorch, Variable has been deprecated, and now the functionality is incorporated into torch.Tensor itself. It supports almost all APIs defined by torch.Tensor and additionally, provides a backward() method that allows easy calculation of gradients.\n",
    "\n",
    "Consider the loss function for a model parameter 'x'. By storing the computed value by the loss function in a tensor 'loss', we can call loss.backward() to compute the gradients ‚àÇloss/‚àÇx for all trainable parameters. PyTorch will store the gradient results back in the corresponding tensor 'x'.\n",
    "\n",
    "Let's delve into a simple demonstration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([[1.9, 2.3, 3.1], [4.1, 5.7, 6.3]], dtype=torch.float32)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we specify that we want to compute gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's square our tensor, ie. let's assume the following functional form.\n",
    "\n",
    "$$\n",
    "f(x) = x^2\n",
    "$$\n",
    "\n",
    "where x is our (2, 3) dimensional tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.mean(tensor*tensor)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's now backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients are now stored in .grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is not the tensor itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting Small üîß: Implementing Logistic Regressions in PyTorch\n",
    "\n",
    "Now that we've acquainted ourselves with tensors and their ability to be automatically differentiated, it's time to apply this knowledge in the context of a basic yet highly effective machine learning model - Logistic Regression. \n",
    "\n",
    "Note that, from now onwards, we will be using problems in the traditional Machine Learning approach:\n",
    "\n",
    "1. Turn your data, whatever it is, into numbers - create a numeric representation.\n",
    "2. Select or build a model to learn the representation as best as possible.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01-machine-learning-a-game-of-two-parts.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "\n",
    "As seen in the slides, a logistic regression is a simple binary classification algorithm that serves as a great entry point to understanding more complex neural networks. In essence, it could be considered as a one-layer neural network with a sigmoid activation function. The sigmoid function transforms the output to fall within the range of 0 to 1, which suits binary classification problems.\n",
    "\n",
    "In the following steps, we will use PyTorch to set up our data and parameters, perform the necessary computations using the sigmoid activation function, and optimise the model through minimising the binary cross-entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a synthetic dataset to demonstrate logistic regression. We use PyTorch's built-in functions to create a tensor of `n_samples` observations, each with `input_size` features. We then assign a binary label to each observation based on the sum of its features. Specifically, we classify an observation as `1` if the sum of its features plus some noise is greater than `0`, and `0` otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some data\n",
    "n_samples = 100\n",
    "input_size = 3\n",
    "\n",
    "x = torch.randn(n_samples, input_size)\n",
    "y = torch.where(x[:, 0] + x[:, 1] + x[:, 2] + 0.5 * torch.randn(n_samples) > 0, 1., 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better understanding of our data, we can visualise it in a 3D scatter plot. Here, each axis corresponds to a feature, and the color of each point represents its class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x[:, 0], x[:, 1], x[:, 2], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our model. We start by initialising the weights and bias. In logistic regression, the weights and bias form the parameters of the model that we need to learn. Each feature in our input data will be multiplied by a corresponding weight, and the bias term allows the model to fit the best hyperplane that separates the classes in the data.\n",
    "\n",
    "<img src=\"https://media5.datahacker.rs/2021/01/44.jpg\" width=\"600\" height=\"300\">\n",
    "\n",
    "\n",
    "The weight tensor is initialised with zeros and has the same number of elements as the input features, and is set to requires_grad=True to enable computation of gradients which is essential for model training. The bias is a scalar, also initialised to zero and set to requires_grad=True.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.zeros(input_size, 1, requires_grad=True)\n",
    "print(weights, weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = torch.zeros(1, requires_grad=True)\n",
    "print(bias, bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two tensors define the parameter space of our logistic regression model. Using these parameters, we will map our input data to a variable z that represents the log-odds of the positive class. The higher the value of z, the higher the predicted probability of the positive class.\n",
    "\n",
    "Next, we define the `sigmoid` function. The sigmoid function maps any real value into another value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the logits (`z`) which is a linear transformation of the features. It's calculated by taking the matrix multiplication of our data (`x`) and the weights, and then adding the bias. We then apply the sigmoid function on the logits to get `y_hat`, the estimated probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.matmul(x, weights) + bias\n",
    "\n",
    "y_hat = sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compute the loss. The loss quantifies how well our predictions `y_hat` match the true values `y`. For logistic regression, we use Binary Cross Entropy (BCE) loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -(y * torch.log(y_hat) + (1 - y) * torch.log(1 - y_hat)).mean()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having made it to the end of the forward pass, we now initiate a backward pass (ie. we backpropagate) and update the weights in `x` given the gradients computed from the loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After backpropagating, we can print the computed gradients for the weights and bias. Recall that each element in `weights.grad` and `bias.grad` represents the partial derivative of the loss function with respect to the corresponding element in `weights` and `bias`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights.grad, bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we update the `weights` and `bias` tensors using gradient descent. However, we need to ensure that these operations do not create computation graph, i.e., the gradient should not be computed for these updates. To achieve this, we perform these operations inside the `torch.no_grad()` context manager, which temporarily disables the autograd mechanism. The learning rate, or speed at which we update our parameters, is set to `0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    weights -= 0.1 * weights.grad\n",
    "    bias -= 0.1 * bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After updating the parameters, it is important to clear the old gradients. By default, PyTorch accumulates gradients, i.e., the next time we call `.backward()`, the new gradient values will be added to the existing gradient values, which will lead to incorrect results. We can reset the gradients back to zero by calling `.zero_()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.grad.zero_()\n",
    "bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's an entire forward and backward pass!\n",
    "\n",
    "Now, we will encapsulate the entire process of a forward and backward pass of logistic regression into a single Python class, `BinaryLogisticRegression`. The class includes the following methods:\n",
    "\n",
    "- `__init__`: Constructor that initialises weights, bias, and learning rate. The weights and bias are set as tensors full of zeros.\n",
    "\n",
    "- `forward`: Performs the forward pass and calculates the predicted output using the input data and parameters.\n",
    "\n",
    "- `sigmoid`: Implements the sigmoid function to squash the output of the forward pass to a probability between 0 and 1.\n",
    "\n",
    "- `loss`: Computes the binary cross entropy loss.\n",
    "\n",
    "- `backward`: Calculates the gradients and updates the weights and bias using gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression:\n",
    "    \"\"\"A simple logistic regression model with binary cross-entropy loss.\n",
    "    \"\"\"    \n",
    "    def __init__(self, input_size, learning_rate=0.1):\n",
    "        self.weights = torch.zeros(input_size, 1, requires_grad=True)\n",
    "        self.bias = torch.zeros(1, requires_grad=True)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.matmul(x, self.weights) + self.bias\n",
    "        return out\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + torch.exp(-z))\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        y_pred = self.sigmoid(y_pred)\n",
    "        loss = -(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred)).mean()\n",
    "        return loss\n",
    "\n",
    "    def backward(self, x, y_true):\n",
    "        y_pred = self.forward(x)\n",
    "        loss = self.loss(y_pred, y_true)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            self.weights -= self.learning_rate * self.weights.grad\n",
    "            self.bias -= self.learning_rate * self.bias.grad\n",
    "            self.weights.grad.zero_()\n",
    "            self.bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now instantiate the `BinaryLogisticRegression` class and train our model for a certain number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryLogisticRegression(input_size=input_size, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In each epoch, the model will perform a forward pass to compute the predicted labels and the loss, then perform a backward pass to compute the gradients and update the weights and bias. The loss for each epoch is printed to monitor the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for some number of epochs\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Compute the predicted labels and the loss\n",
    "    y_pred = model.forward(x)\n",
    "    loss = model.loss(y_pred, y)\n",
    "\n",
    "    # Compute the gradients and update the weights and bias\n",
    "    model.backward(x, y)\n",
    "\n",
    "    # Print the loss for this epoch\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}: Loss = {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using PyTorch's `nn` module üî≠\n",
    "\n",
    "In our previous version of the logistic regression model, we manually defined the model parameters (weights and bias), the sigmoid activation function, and the binary cross-entropy loss function. We then manually computed the forward pass, the loss, the gradients via backpropagation, and updated the model parameters.\n",
    "\n",
    "This process, while illustrative, can be greatly simplified and made more readable by using PyTorch's nn module. This module provides a high-level, object-oriented API for defining neural networks, sort of the lego blocks of NNs. It includes classes for many common types of layers (like linear layers), activation functions (like sigmoid), and loss functions (like binary cross-entropy), among other things.\n",
    "\n",
    "`torch.nn` is among a set of essential modules that in conjunction can be used to create almost any kind of neural network. The ingredients are the following:\n",
    "\n",
    "| PyTorch module | What does it do? |\n",
    "| ----- | ----- |\n",
    "| [`torch.nn`](https://pytorch.org/docs/stable/nn.html) | Contains all of the building blocks for computational graphs (essentially a series of computations executed in a particular way). |\n",
    "| [`torch.nn.Parameter`](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter) | Stores tensors that can be used with `nn.Module`. If `requires_grad=True` gradients (used for updating model parameters via [**gradient descent**](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html))  are calculated automatically, this is often referred to as \"autograd\".  | \n",
    "| [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) | The base class for all neural network modules, all the building blocks for neural networks are subclasses. If you're building a neural network in PyTorch, your models should subclass `nn.Module`. Requires a `forward()` method be implemented. | \n",
    "| [`torch.optim`](https://pytorch.org/docs/stable/optim.html) | Contains various optimisation algorithms (these tell the model parameters stored in `nn.Parameter` how to best change to improve gradient descent and in turn reduce the loss). | \n",
    "| `def forward()` | All `nn.Module` subclasses require a `forward()` method, this defines the computation that will take place on the data passed to the particular `nn.Module` (e.g. the linear regression formula above). |\n",
    "\n",
    "Let's now rewrite our logistic regression model using the nn module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BinaryLogisticRegression(nn.Module):\n",
    "    \"\"\"A simple logistic regression model.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, n_classes):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, n_classes) # This is equivalent to self.weights * x + self.bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.sigmoid(self.linear(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we've changed the output to take two-item tensor rows, corresponding to class 0 and 1, respectively. Let's also change our `y` to accomodate for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oy = y.clone().detach()\n",
    "\n",
    "# Create a new tensor of zeros with the desired shape\n",
    "y = torch.zeros((y.size(0), 2))\n",
    "\n",
    "# Fill the new tensor with 1s at the indices specified by the original tensor\n",
    "y.scatter_(1, oy.long().unsqueeze(1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the `nn.Module` base class makes it easy to define our model as a class, and the `nn.Linear` class abstracts away the need to manually define and initialise weights and bias.\n",
    "\n",
    "We can similarly use `nn.BCELoss` to compute the binary cross-entropy loss, and use a PyTorch optimiser from the `torch.optim` module to update our model parameters during training, which abstracts away the manual computation of gradients and the updating of model parameters. These reduce the coding overheads, and can be summarised as follows:\n",
    "\n",
    "| Function | What does it do? | Where does it live in PyTorch? | Common values |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| **Loss function** | Measures how wrong your models predictions (e.g. `y_preds`) are compared to the truth labels (e.g. `y_test`). Lower the better. | PyTorch has plenty of built-in loss functions in [`torch.nn`](https://pytorch.org/docs/stable/nn.html#loss-functions). | Mean absolute error (MAE) for regression problems ([`torch.nn.L1Loss()`](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html)). Binary cross entropy for binary classification problems ([`torch.nn.BCELoss()`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)).  |\n",
    "| **Optimiser** | Tells your model how to update its internal parameters to best lower the loss. | You can find various optimization function implementations in [`torch.optim`](https://pytorch.org/docs/stable/optim.html). | Stochastic gradient descent ([`torch.optim.SGD()`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)). Adam optimizer ([`torch.optim.Adam()`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)). | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryLogisticRegression(input_size=input_size, n_classes=2)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the model using the steps we've seen before.\n",
    "\n",
    "| Number | Step name | What does it do? | Code example |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| 1 | Forward pass | The model goes through all of the training data once, performing its `forward()` function calculations. | `model(x_train)` |\n",
    "| 2 | Calculate the loss | The model's outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are. | `loss = loss_fn(y_pred, y_train)` | \n",
    "| 3 | Zero gradients | The optimisers gradients are set to zero (they are accumulated by default) so they can be recalculated for the specific training step. | `optimizer.zero_grad()` |\n",
    "| 4 | Perform backpropagation on the loss | Computes the gradient of the loss with respect for every model parameter to be updated  (each parameter with `requires_grad=True`). This is known as **backpropagation**, hence \"backwards\".  | `loss.backward()` |\n",
    "| 5 | Update the optimiser (**gradient descent**) | Update the parameters with `requires_grad=True` with respect to the loss gradients in order to improve them. | `optimizer.step()` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of the logistic regression model using the nn module is not only shorter and more readable, but it also takes full advantage of PyTorch's capabilities for automatic differentiation, making it easier to build more complex models. As an aside, note the use of stochastic gradient descent has allowed for much quicker convergence.\n",
    "\n",
    "Let's plot the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some predictions\n",
    "y_pred = model(x)\n",
    "y_pred_labels = torch.max(y_pred.detach(), dim=1)[1]  # Get the index of the max log-probability.\n",
    "\n",
    "# Visualize the true and predicted labels\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot for true labels\n",
    "scatter_true = ax.scatter(x[:, 0], x[:, 1], x[:, 2], c=torch.max(y, 1)[1], marker='o')\n",
    "# Scatter plot for predicted labels\n",
    "scatter_pred = ax.scatter(x[:, 0], x[:, 1], x[:, 2], c=y_pred_labels, marker='x')\n",
    "\n",
    "plt.legend([scatter_true, scatter_pred], ['True Labels', 'Predicted Labels'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the performance of the model on unseen data. A testing loop is quite similar to a training loop, except that we don't compute gradients or update the model parameters. Instead, we compute the predictions for the test data and compare them to the true labels to evaluate the model's performance. We will pass `torch.eval` to turn off any training functionality during each epoch, and enter the `torch.inference_mode` context manager to disable gradient tracking for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say we have some additional test data `x_test` and `y_test`\n",
    "x_test = torch.randn(20, input_size)\n",
    "y_test = torch.nn.functional.one_hot(torch.where(x_test[:, 0] + x_test[:, 1] + x_test[:, 2] + 0.5 * torch.randn(20) > 0, 1, 0)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryLogisticRegression(input_size=input_size, n_classes=2)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin the training loop. For each epoch, we will perform forward propagation, compute the loss, perform backpropagation, and update the model parameters. We will also evaluate the model's performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "train_loss_values = []\n",
    "test_loss_values = []\n",
    "epoch_count = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # testing \n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        test_outputs = model(x_test)\n",
    "        test_loss = criterion(test_outputs, y_test)\n",
    "\n",
    "    # append to plot\n",
    "    epoch_count.append(epoch)\n",
    "    train_loss_values.append(loss.detach().numpy())\n",
    "    test_loss_values.append(test_loss.detach().numpy())\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, MAE Train Loss: {loss.item()}, MAE Test Loss: {test_loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the training is done, let's plot the training and testing loss for each epoch. This will help us understand the model's learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curves\n",
    "plt.plot(epoch_count, train_loss_values, label=\"Train loss\")\n",
    "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
    "plt.title(\"Training and test loss curves\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll use the trained model to make predictions on the training and test data. Let's also visualise the true and predicted labels in a 3D scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some predictions\n",
    "y_pred = model(x)\n",
    "y_pred_labels = torch.argmax(y_pred.detach(), dim=1)  # find class index with max value\n",
    "\n",
    "y_test_pred = model(x_test).detach()\n",
    "y_test_pred_labels = torch.argmax(y_test_pred.detach(), dim=1)  # find class index with max value\n",
    "\n",
    "# Visualize the true and predicted labels\n",
    "fig = plt.figure()\n",
    "axs0 = fig.add_subplot(121, projection='3d')\n",
    "axs1 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "# Scatter plot for true labels\n",
    "scatter_true = axs0.scatter(x[:, 0], x[:, 1], x[:, 2], c=torch.argmax(y, dim=1), marker='o')  # use class index\n",
    "# Scatter plot for predicted labels\n",
    "scatter_pred = axs0.scatter(x[:, 0], x[:, 1], x[:, 2], c=y_pred_labels, marker='x')\n",
    "\n",
    "# Scatter plot for true labels\n",
    "scatter_test_true = axs1.scatter(x_test[:, 0], x_test[:, 1], x_test[:, 2], c=torch.argmax(y_test, dim=1), marker='o')  # use class index\n",
    "# Scatter plot for predicted labels\n",
    "scatter_test_pred = axs1.scatter(x_test[:, 0], x_test[:, 1], x_test[:, 2], c=y_test_pred_labels, marker='x')\n",
    "\n",
    "plt.legend([scatter_true, scatter_pred], ['True Labels', 'Predicted Labels'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving and loading a PyTorch model üíæ\n",
    "\n",
    "Naturally, it is possible to save and load PyTorch models. This is crucial when you need to save the state of your trained model for future use without having to retrain it, or if you want to share your model so others can reproduce your work. \n",
    "\n",
    "PyTorch provides us with three key methods to perform these operations:\n",
    "\n",
    "| PyTorch method | What does it do? | \n",
    "| ----- | ----- |\n",
    "| [`torch.save`](https://pytorch.org/docs/stable/torch.html?highlight=save#torch.save) | Saves a serialised object to disk using Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. Models, tensors and various other Python objects like dictionaries can be saved using `torch.save`.  | \n",
    "| [`torch.load`](https://pytorch.org/docs/stable/torch.html?highlight=torch%20load#torch.load) | Uses `pickle`'s unpickling features to deserialise and load pickled Python object files (like models, tensors or dictionaries) into memory. You can also set which device to load the object to (CPU, GPU etc). |\n",
    "| [`torch.nn.Module.load_state_dict`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict#torch.nn.Module.load_state_dict)| Loads a model's parameter dictionary (`model.state_dict()`) using a saved `state_dict()` object. | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand these methods, let's proceed to see them in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Logistic Regression to Neural Networks\n",
    "\n",
    "We have been using logistic regression as a tool to understand the building blocks of PyTorch: tensors, automatic differentiation, loss functions, and the training process. \n",
    "\n",
    "However, logistic regression is quite limited. It can only learn a linear boundary between classes, and it assumes that the input features are independent of each other. These limitations make it unsuitable for many real-world tasks, where the relationship between features and target variable is often complex and non-linear.\n",
    "\n",
    "A more powerful and flexible model to handle such tasks is a neural network. The structure of a neural network allows us to learn more complicated patterns. At its core, a neural network is simply a stacked version of the logistic regression units we have seen so far. But instead of having a single linear layer followed by an activation function (like we did in logistic regression), in a neural network we have many such layers, hence the term 'deep' learning.\n",
    "\n",
    "This structure, along with different activation functions, gives neural networks the ability to approximate any function, given enough neurons and layers. This is what's called the [Universal Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem).\n",
    "\n",
    "Let's illustrate what we just said. We started from a linear regression, where we had a linear relationship between the inputs and the output:\n",
    "\n",
    "$$\n",
    "y = w * x + b\n",
    "$$\n",
    "\n",
    "Then we moved to logistic regression, where we added a sigmoid activation function to squash the output between 0 and 1:\n",
    "\n",
    "$$\n",
    "y = \\sigma(w * x + b)\n",
    "$$\n",
    "\n",
    "Now in a neural network, we just stack multiple of these logistic units together. If we have two layers, it would look something like this:\n",
    "\n",
    "$$\n",
    "h = \\sigma(w_{1} * x + b_{1}) \\\\\n",
    "y = \\sigma(w_{2} * h + b_{2})\n",
    "$$\n",
    "\n",
    "We can have as many hidden layers as we want, with as many neurons in each layer as we want, and we can use different activation functions (ReLU, tanh, etc.) to introduce non-linearities in the model. This makes neural networks extremely versatile and powerful.\n",
    "\n",
    "\n",
    "<img src=\"https://www.ibm.com/blog/wp-content/uploads/2023/07/deep-neural-network.png\" width=\"600\" height=\"400\">\n",
    "\n",
    "##### Different Activation Functions \n",
    "\n",
    "Activation functions are used to introduce non-linearity in the neural network, helping it to learn from the complex patterns in the data. When we do not use a activation function in the network, its output is simply a linear transformation of the input. Linear transformations are easy to compute and analyse, but they are not able to capture complex patterns and structures in the data.\n",
    "\n",
    "Let's take a look at some of the commonly used activation functions in neural networks:\n",
    "\n",
    "- **Sigmoid**: It maps the input values between 0 and 1, providing a clear distinction between outputs. It's useful in the last layer of a binary classification network, but it is seldom used in hidden layers because of problems like vanishing gradients.\n",
    "\n",
    "- **Softmax**: Softmax is the multi-class generalisation of the sigmoid function for classification problems. It generates the probabilities for the output, and the probabilities sum will be 1.\n",
    "\n",
    "- **Tanh**: The tanh function maps the input values between -1 and 1. It is zero-centered, making it easier to model inputs that have strongly negative, neutral, and strongly positive values.\n",
    "\n",
    "- **ReLU (Rectified Linear Unit)**: ReLU is the most commonly used activation function in neural networks. It gives an output x if x is positive and 0 otherwise. ReLU is linear for all the positive values, and zero for all the negative values, providing the ability to activate neurons based on the positivity of input.\n",
    "\n",
    "- **Leaky ReLU**: It is a variant of ReLU. Instead of being zero when x < 0, a leaky ReLU allows a small, non-zero gradient Œ±. The idea is to prevent the \"dying ReLU\" problem where a neuron never activates again because it always falls in the zero region of ReLU during the training.\n",
    "\n",
    "In the next section, we'll introduce how to create a simple feed-forward neural network in PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Neural Network in PyTorch\n",
    "\n",
    "In the previous sections, we have discussed about the limitations of logistic regression and how neural networks, with their ability to learn complex patterns and structures, offer a way to address these limitations. The introduction of different activation functions further enhances the learning capacity of these networks. \n",
    "\n",
    "Now, let's get hands-on with PyTorch again and learn how to construct a simple feed-forward neural network, also known as a Multi-Layer Perceptron (MLP). \n",
    "\n",
    "We'll be using PyTorch's `nn.Module` again, which provides a clean and flexible way to define complex models. We'll also see how to use different activation functions within our model. \n",
    "\n",
    "In the MLP we're about to construct, the input passes through multiple layers of neurons. Each layer is fully connected to the next one, and data flows from input to output without looping back. These characteristics give this type of network its \"feed-forward\" name. \n",
    "\n",
    "Let's start coding our first neural network!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # convenience\n",
    "\n",
    "class BasicNeuralNetwork(nn.Module):\n",
    "    \"\"\"A basic neural network model.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, n_classes):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, 3)\n",
    "        self.linear2 = nn.Linear(3, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the neural network.\n",
    "        \"\"\"\n",
    "        x = self.linear1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.linear2(x)\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're still working with our first binary classification example. As such, we'll still employ Binary Cross Entropy to compute our loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicNeuralNetwork(input_size=input_size, n_classes=2)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "train_loss_values = []\n",
    "test_loss_values = []\n",
    "epoch_count = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # testing \n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        test_outputs = model(x_test)\n",
    "        test_loss = criterion(test_outputs, y_test)\n",
    "\n",
    "    # append to plot\n",
    "    epoch_count.append(epoch)\n",
    "    train_loss_values.append(loss.detach().numpy())\n",
    "    test_loss_values.append(test_loss.detach().numpy())\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, MAE Train Loss: {loss.item()}, MAE Test Loss: {test_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curves\n",
    "plt.plot(epoch_count, train_loss_values, label=\"Train loss\")\n",
    "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
    "plt.title(\"Training and test loss curves\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the potential of Neural Networks to model complex relationships and interactions among features, our basic three-layered neural network model performed worse than the initial binary logistic regression model for the same number of epochs. The reason for this degradation in performance is likely due to the complexity of the neural network model and the relatively small size of our dataset.\n",
    "\n",
    "Our neural network model has many more parameters than the logistic regression model. These parameters need to learn the structure in the data to make accurate predictions. However, with a small dataset, the amount of structure or information available to learn from is limited. This can cause the model to overfit, where it learns the noise in the training set instead of the underlying structure of the data, resulting in poor generalisation to unseen data (i.e., test set).\n",
    "\n",
    "Beyond data concerns, there are several levers one can pull to improve model performance. These are presented below (credit goes to Daniel Bourke):\n",
    "\n",
    "| Model improvement technique* | What does it do? |\n",
    "| ----- | ----- |\n",
    "| **Add more layers** | Each layer *potentially* increases the learning capabilities of the model with each layer being able to learn some kind of new pattern in the data, more layers is often referred to as making your neural network *deeper*. |\n",
    "| **Add more hidden units** | Similar to the above, more hidden units per layer means a *potential* increase in learning capabilities of the model, more hidden units is often referred to as making your neural network *wider*. |\n",
    "| **Fitting for longer (more epochs)** | Your model might learn more if it had more opportunities to look at the data. |\n",
    "| **Changing the activation functions** | Some data just can't be fit with only straight lines (like what we've seen), using non-linear activation functions can help with this (hint, hint). |\n",
    "| **Change the learning rate** | Less model specific, but still related, the learning rate of the optimiser decides how much a model should change its parameters each step, too much and the model overcorrects, too little and it doesn't learn enough. |\n",
    "| **Change the loss function** | Again, less model specific but still important, different problems require different loss functions. For example, a binary cross entropy loss function won't work with a multi-class classification problem. |\n",
    "| **Use transfer learning** | Take a pretrained model from a problem domain similar to yours and adjust it to your own problem. We cover transfer learning in [notebook 06](https://www.learnpytorch.io/06_pytorch_transfer_learning/). |\n",
    "\n",
    "An **amazing** resource to practice how these hyperparameters affect model performance is Tensorflow's [Playground](https://playground.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Branching out: Regression and classification with different data types üìè\n",
    "\n",
    "What we've seen so far can be applied across a broad range of problems in machine learning. From simple binary classification, we extended our problem to multi-class classification with a more complex neural network model. In doing so, we've primarily dealt with simple, tabular data, where each instance or observation in our dataset is represented by a fixed set of features. However, the beauty of machine learning, and deep learning in particular, is that these same principles can be applied to vastly different types of data and problems.\n",
    "\n",
    "Let's begin with a multi-class classification problem that uses text as input data.\n",
    "\n",
    "##### 1. Surname Origin multi-class prediction üìï\n",
    "\n",
    "In this section, we will explore a multi-class prediction problem, where the objective is to predict the origin of a surname based on the character sequences that it contains. By treating each surname as a sequence of characters, we can leverage a range of sequence learning techniques to build our predictive model. The task is to learn a mapping from sequences of characters to a specific nationality, effectively learning the implicit rules that often determine how surnames are formed in different cultures. This type of predictive task has broad applications, from natural language processing to bioinformatics, and provides an interesting example of how sequence data can be effectively handled using modern machine learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surnames = getter.get_surnames()\n",
    "surnames.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 2))\n",
    "\n",
    "X = vectorizer.fit_transform(surnames['surnames'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_bicharacters\n",
    "plot_bicharacters(vectorizer, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by transforming the 2-character combinations into a tensor. Then, we will one-hot encode the nationalities for our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X.toarray(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(surnames['country'])\n",
    "y = torch.tensor(y, dtype=torch.long) # Note we're not using (n_samples, n_classes) here. If you want, you can do it using F.one_hot, and changing the loss function to BCEWithLogitsLoss for one-to-many cross-entropies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then split the data into training and test sets. We'll use 80% of the data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set our hyperparameters for the network such as the input size, output size, hidden layer size, batch size, learning rate, and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X.shape[1] # Number of unique bigrams\n",
    "output_size = 3 # Number of nationalities\n",
    "hidden_size = 100 # Size of hidden layer\n",
    "batch_size = 8 # Number of samples to process at a time - useful for computational efficiency, memory usage, generalisation, etc.\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a DataLoader that will help us handle the data efficiently during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the structure of our neural network. It will have one hidden layer with ReLU activation, and the output layer will not have an activation function since we will be using the CrossEntropyLoss which combines LogSoftmax and NLLLoss in one single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using CrossEntropyLoss as our loss function and Adam as our optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  # Adam loss! Faster convergence, possibly worse results than SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the network for a certain number of epochs, where in each epoch we use the DataLoader to get batches of data and feed them into our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (bigrams, labels) in enumerate(train_loader):\n",
    "        outputs = model(bigrams)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Batch {i}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predicted = []\n",
    "    for bigrams, labels in test_loader:\n",
    "        outputs = model(bigrams)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        all_predicted += predicted.tolist()\n",
    "\n",
    "    print(f'Accuracy: {100 * correct / total}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a confusion matrix to visualise how well our model performed for each nationality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, all_predicted)\n",
    "plt.figure(figsize=(4,4))\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Panel Data Regression - Bike Sharind Demand prediction üö¥\n",
    "\n",
    "Let's shift gears to regression problems using panel data. Panel data, also known as longitudinal data, are multi-dimensional data involving measurements over time. The popular Bike Sharing Demand dataset provides an ideal case for such an analysis. The dataset comprises hourly rental data spanning two years from a bike sharing service in Washington, D.C., USA. The data includes environmental and seasonal variables, providing rich, multi-dimensional, time-series data. The objective is to predict the count of total bike rentals at each hour based on these variables, essentially a time-series regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_sharing = getter.get_bike_demand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_sharing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common technique in time-series analysis is to use lagged variables as predictors. A lagged variable is a variable that has been shifted forward in time. For example, to predict the bike count at a given hour, we could use the bike counts from the previous few hours as predictors. These lagged variables can capture temporal patterns and trends in the data, such as daily or weekly cycles in bike demand.\n",
    "\n",
    "Let's proceed with pre-processing the data, creating lagged variables, and splitting the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_sharing['dteday'] = pd.to_datetime(bike_sharing['dteday'])\n",
    "\n",
    "# Create lagged variables\n",
    "for i in range(1, 4):\n",
    "    bike_sharing[f'lag_{i}'] = bike_sharing['cnt'].shift(i)\n",
    "\n",
    "# Drop rows with missing values\n",
    "bike_sharing.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count of rented bikes can depend on factors such as the time of the day, day of the week, or even the season of the year. Because of these temporal dependencies, it's not appropriate to perform a random train-test split, as is done in non-time-series tasks. Doing so could result in temporal leakage, where information from the future could be used to predict the past.\n",
    "\n",
    "Instead, we perform a chronological split of the data. The training set consists of the earliest data points, and the test set consists of the latest data points. This way, our model is trained on past data and tested on future data, as would be the case in real-world prediction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = bike_sharing[bike_sharing['dteday'] < pd.to_datetime('2012-10-01')]\n",
    "test = bike_sharing[bike_sharing['dteday'] >= pd.to_datetime('2012-10-01')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the train and test sets into PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(train.drop(columns=['dteday', 'cnt']).values, dtype=torch.float)\n",
    "y_train = torch.tensor(train['cnt'].values, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "X_test = torch.tensor(test.drop(columns=['dteday', 'cnt']).values, dtype=torch.float)\n",
    "y_test = torch.tensor(test['cnt'].values, dtype=torch.float).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also normalise the features, which often leads to better training dynamics and less extreme values in your activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = X_train.mean(dim=0)\n",
    "std = X_train.std(dim=0)\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the loader objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define our neural network. We'll make this network slightly larger, on account of the higher dimensional feature space and large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BikeSharingNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(X_train.shape[1], 64)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.layer3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "    \n",
    "model = BikeSharingNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the last layer has no activation function. In this case, we'll also select a loss criterion that doesn't perform non-linear transformations. Instead, due to this being a regression exercise, we can seek to minimise the square error of the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing to do but train the model now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "for epoch in range(num_epochs):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, targets = batch\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Batch {i}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And test its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "\n",
    "# Calculate the root mean square error (RMSE)\n",
    "rmse = torch.sqrt(criterion(predictions, y_test)).item()\n",
    "print(f'Test RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:, \"predictions\"] = model(X_train).detach().numpy()\n",
    "test.loc[:, \"predictions\"] = model(X_test).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our predictions. Now, hourly rates make for poor visualisation, so let's aggregate to the day level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_sharing = bike_sharing.groupby([\"dteday\"]).agg({\"cnt\": \"mean\"}).reset_index()\n",
    "train = train.groupby([\"dteday\"]).agg({\"cnt\": \"mean\", \"predictions\": \"mean\"}).reset_index()\n",
    "test = test.groupby([\"dteday\"]).agg({\"cnt\": \"mean\", \"predictions\": \"mean\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_sharing = bike_sharing[bike_sharing.dteday >= \"2012-06-01\"]\n",
    "train = train[train.dteday >= \"2012-06-01\"]\n",
    "\n",
    "# Define the line for true values\n",
    "true_line = alt.Chart(bike_sharing).mark_line().encode(\n",
    "    x='dteday:T',\n",
    "    y='cnt:Q',\n",
    "    color=alt.value('black')\n",
    ")\n",
    "\n",
    "# Define the line for training predictions\n",
    "train_line = alt.Chart(train).mark_line(strokeDash=[5,5]).encode(\n",
    "    x='dteday:T',\n",
    "    y='predictions:Q',\n",
    "    color=alt.value('blue')\n",
    ")\n",
    "\n",
    "# Define the line for test predictions\n",
    "test_line = alt.Chart(test).mark_line(strokeDash=[5,5]).encode(\n",
    "    x='dteday:T',\n",
    "    y='predictions:Q',\n",
    "    color=alt.value('green')\n",
    ")\n",
    "\n",
    "# Combine all three lines into a single plot\n",
    "alt.layer(true_line, train_line, test_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Estimating Estonian Scenes: A Binary-class image classification problem üè†/üå≤\n",
    "\n",
    "In this example, we will approach an image classification problem. We will be working with a set of images that depict either Estonian cities or Estonian nature scenes. Our goal is to develop a model capable of identifying which type of scene each image presents.\n",
    "\n",
    "The data we are dealing with comprises 100 images, each of 224 x 224 x 3 pixels, meaning we have a rather high-dimensional problem on our hands. Let's get on coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = getter.get_images()\n",
    "image_labels = getter.get_image_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [i[1] for i in images]\n",
    "labels = image_labels.iloc[:20][\"class\"].tolist()\n",
    "\n",
    "fig = plt.figure(figsize=(20, 1))\n",
    "for i, (image_file, image_label) in enumerate(zip(images[:20], labels[:20])):  \n",
    "    plt.subplot(1, 20, i+1)\n",
    "    plt.title(image_label)\n",
    "    plt.imshow(image_file)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use some of the modules from `torchvision`, a library that contains many useful tools and methods for wrangling image data. In this case, we will simply define the pipeline that sets the 224 x 224 x 3 tensor and rescales the [0,255] values to be between [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define a transformer to convert PIL image to PyTorch tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()  # convert to tensor and scale to [0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.stack([transform(image) for image in images])\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go back to the familiar set of steps we've seen twice already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output label tensor\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(image_labels['class'])\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create dataloaders\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
    "test_ds = TensorDataset(X_test, y_test)\n",
    "test_dl = DataLoader(test_ds, batch_size=4, shuffle=True)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's proceed to build our model. Notice that our architecture employs traditional, fully-connected layers, a design choice that causes us to forego any spatial relationship between the pixels. Instead, we opt to flatten the 224 x 224 x 3 image inputs into a long 224 * 224 * 3 vector. This approach may seem counterintuitive, especially considering that images inherently possess spatial information that could be crucial to understanding and interpreting them. Indeed, you may start to suspect that fully connected networks are not the most efficient or effective method to tackle image-based tasks - and you'd be right. \n",
    "\n",
    "Conventional neural networks tend to perform poorly on image data due to this lack of spatial consideration. In upcoming sections, we will introduce Convolutional Neural Networks (CNNs), a type of network that is specifically designed for image processing and preserves spatial relationships, thereby achieving significantly better performance. However, for this simple case, we will stick with a standard fully connected network to establish the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Input tensor has shape (batch_size, 3, 224, 224)\n",
    "        self.flatten = nn.Flatten()  # flatten the input\n",
    "        self.layer1 = nn.Linear(3 * 224 * 224, 256)  # first hidden layer\n",
    "        self.layer2 = nn.Linear(256, 64)  # second hidden layer\n",
    "        self.layer3 = nn.Linear(64, 32)  # output layer (2 classes)\n",
    "        self.layer4 = nn.Linear(32, 2)  # output layer (2 classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)  # Flatten the input\n",
    "        x = F.relu(self.layer1(x))  # First hidden layer with ReLU activation\n",
    "        x = F.relu(self.layer2(x))  # Second hidden layer with ReLU activation\n",
    "        x = F.relu(self.layer3(x))  # Third layer with ReLU activation\n",
    "        x = self.layer4(x)  # Output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function and optimizer\n",
    "model = ImageNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important aside is [TensorBoard](https://pytorch.org/docs/stable/tensorboard.html), a visualisation tool that can help us track and understand the training process of our neural networks. Originally designed for TensorFlow, it's compatible with PyTorch and offers various functionalities: monitoring training progress, comparing runs, visualising model structures, and more.\n",
    "\n",
    "In this image classification example, we could leverage TensorBoard to track our loss and accuracy over time, compare the performance of different architectures, and even visualise the model's feature maps. By logging this data during training using a SummaryWriter, and running TensorBoard pointing to our logging directory, we would have a dynamic and interactive way to inspect and understand our model's behavior, which can greatly aid in debugging and improving our model.\n",
    "\n",
    "A very good tutorial exists [here](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html) to delve deeper in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()\n",
    "writer.add_image('images', grid, 0)\n",
    "writer.add_graph(model, images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now go back to training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "n_epochs = 20  # Number of epochs\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for images, labels in train_dl:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # compute loss for test\n",
    "        with torch.inference_mode():\n",
    "            test_loss = criterion(model(X_test), y_test)\n",
    "\n",
    "        # If you have tensorflow installed, you can uncomment the following line to see the progress updates\n",
    "        writer.add_scalar('Loss/train', loss.item(), epoch)\n",
    "        writer.add_scalar('Loss/test', test_loss.item(), epoch)\n",
    "        \n",
    "    print(f'Epoch {epoch+1}/{n_epochs}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the network\n",
    "correct = 0\n",
    "total = 0\n",
    "misclassified_images = []\n",
    "misclassified_labels = []\n",
    "correct_images = []\n",
    "correct_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dl:\n",
    "        images, labels = data\n",
    "        outputs = model(images.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Check for misclassifications\n",
    "        for image, label, prediction in zip(images, labels, predicted):\n",
    "            if label != prediction:\n",
    "                misclassified_images.append(image)\n",
    "                misclassified_labels.append(label.item())\n",
    "                predicted_labels.append(prediction.item())\n",
    "            else:\n",
    "                correct_images.append(image)\n",
    "                correct_labels.append(label.item())\n",
    "\n",
    "print('Accuracy of the network on test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to tensors for easier manipulation\n",
    "misclassified_images = torch.stack(misclassified_images)\n",
    "misclassified_labels = torch.tensor(misclassified_labels)\n",
    "predicted_labels = torch.tensor(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "for i in range(len(misclassified_images)):\n",
    "    ax = fig.add_subplot(4, 5, i+1)  # Assuming a maximum of 20 misclassifications\n",
    "    img = misclassified_images[i].permute(1, 2, 0).numpy()  # convert to numpy and rearrange dimensions\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"True: {misclassified_labels[i]}, Predicted: {predicted_labels[i]}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "for i in range(len(correct_images)):\n",
    "    ax = fig.add_subplot(4, 5, i+1)  # Assuming a maximum of 20 misclassifications\n",
    "    img = correct_images[i].permute(1, 2, 0).numpy()  # convert to numpy and rearrange dimensions\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"True: {correct_labels[i]}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our analysis, it is apparent that the model struggled to effectively discriminate between images of Estonian cities and nature. Rather intriguingly, the model seemed to have learned to always predict the 'nature' class. The slight imbalance in our training data ‚Äî favoring nature images ‚Äî might have influenced this tendency. By always predicting 'nature', the model minimised its loss, as it was indeed correct more often.\n",
    "\n",
    "This behavior illustrates a fundamental limitation of using fully connected layers for image classification tasks. In such networks, the spatial structure and locality of the input are not taken into consideration, and each pixel is treated independently. In the context of image processing, this disregards crucial information: the relationship between a pixel and its neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Networks\n",
    "\n",
    "To capture the spatial relationships mentioned above, we can use Convolutional Neural Networks (CNNs), a type of Neural Network that is specifically designed to process grid-like data, such as images. Unlike standard Neural Networks, CNNs maintain the spatial relationship between pixels by learning internal feature representations using small squares of input data. This is accomplished through the use of specialised layers (Convolutional Layers, Pooling Layers) before flattening the data and passing it to Fully Connected Layers.\n",
    "\n",
    "![](https://www.researchgate.net/profile/Serkan_Kiranyaz/publication/313676923/figure/fig2/AS:461584393871361@1487061703456/A-standard-2D-CNN-10.png)\n",
    "\n",
    "CNNs have a couple of advantages over standard Neural Networks for image processing. First, they are translation invariant, ie., they can recognise patterns regardless of where they are located in the image. Second, they can capture the local patterns in images (like shapes, textures, etc.), making them especially effective for tasks like image classification. For a good visual explanation, see [this interactive page](https://poloclub.github.io/cnn-explainer/).\n",
    "\n",
    "We already introduced all other tools we use in PyTorch when working with image data, including `torchvision` and `torchvision.transforms`. Let's briefly present convolutional layers and pooling layers, and then build the model and train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convolutional Layers\n",
    "\n",
    "The Convolutional layer is the main building block of a Convolutional Neural Network (CNN). The layer's parameters consist of a set of learnable filters, which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume and computes dot products between the entries of the filter and the input, producing an activation map.\n",
    "\n",
    "![](https://thigiacmaytinh.com/wp-content/uploads/2018/05/kernel.png)\n",
    "\n",
    "As a result, the network learns filters that activate when they see certain types of features at some spatial position in the input.\n",
    "\n",
    "For example, let's say we have a 5x5 grayscale image (i.e., each pixel is a single number), and we use a 3x3 filter, the convolution operation might look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5x5 image\n",
    "image = np.array([[1, 1, 1, 0, 0],\n",
    "                  [0, 1, 1, 1, 0],\n",
    "                  [0, 0, 1, 1, 1],\n",
    "                  [0, 0, 1, 1, 0],\n",
    "                  [0, 1, 1, 0, 0]])\n",
    "\n",
    "# 3x3 filter\n",
    "filt = np.array([[1, 0, 1],\n",
    "                 [0, 1, 0],\n",
    "                 [1, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_convolution\n",
    "\n",
    "results = plot_convolution(image, filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Max-Pooling layers\n",
    "![](https://morvanzhou.github.io/static/results/ML-intro/cnn5.png)\n",
    "\n",
    "Early papers on CNNs found that at each convolution, the neural layer would inadvertently lose some information. A popular solution is to pooling results, which also help reduce dimensionality in the follow-up layers. Let's quickly visualise this using the example from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_pooling\n",
    "\n",
    "plot_pooling(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building a Convolutional Neural Network\n",
    "\n",
    "We've just seen how a convolutional layer works - it applies a set of filters to different regions of the input, preserving spatial relationships between pixels. We've also seen how a max pooling layer works, reducing the dimensionality of the input while preserving the most important information (the maximum values in different regions).\n",
    "\n",
    "A Convolutional Neural Network (CNN) typically consists of a series of convolutional layers, each followed by a non-linear activation function such as ReLU, and often followed by a pooling layer. This sequence can be repeated multiple times, creating a deep stack of layers. The idea is to use convolutional layers to extract increasingly complex features from the input image, while reducing dimensionality with pooling layers.\n",
    "\n",
    "After this series of convolutional and pooling layers, the output is flattened into a 1D vector, and one or more fully connected layers (like in a traditional Neural Network) are used for final classification. It's important to note that these fully connected layers see the entire image at once, so they can learn to recognize combinations of features extracted by the convolutional layers, regardless of where in the image they were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1) # Convolutional layer (sees 224x224x3 image tensor)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1) # Convolutional layer (sees 112x112x16 tensor after max pooling)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1) # Convolutional layer (sees 56x56x32 tensor after max pooling)\n",
    "        self.pool = nn.MaxPool2d(2, 2) # Max pooling layer\n",
    "        self.fc1 = nn.Linear(64 * 28 * 28, 500) # Linear layer (64 * 28 * 28 -> 500)\n",
    "        self.fc2 = nn.Linear(500, 2) # Linear layer (500 -> 2)\n",
    "        self.dropout = nn.Dropout(0.25) # Dropout layer (p=0.25) - A new addition to prevent overfitting\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 28 * 28) # Flatten image input\n",
    "        x = self.dropout(x) # Add dropout layer\n",
    "        x = F.relu(self.fc1(x)) # Add 1st hidden layer, with relu activation function\n",
    "        x = self.dropout(x) # Add dropout layer\n",
    "        x = self.fc2(x) # Add 2nd hidden layer, with relu activation function\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let's instantiate the model and define our loss and optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageCNN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs to train the model\n",
    "n_epochs = 40\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for data, target in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "    train_loss = train_loss/len(train_dl.dataset)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare results to the previous MLP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the network\n",
    "correct = 0\n",
    "total = 0\n",
    "misclassified_images = []\n",
    "misclassified_labels = []\n",
    "correct_images = []\n",
    "correct_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dl:\n",
    "        images, labels = data\n",
    "        outputs = model(images.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Check for misclassifications\n",
    "        for image, label, prediction in zip(images, labels, predicted):\n",
    "            if label != prediction:\n",
    "                misclassified_images.append(image)\n",
    "                misclassified_labels.append(label.item())\n",
    "                predicted_labels.append(prediction.item())\n",
    "            else:\n",
    "                correct_images.append(image)\n",
    "                correct_labels.append(label.item())\n",
    "\n",
    "print('Accuracy of the network on test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert lists to tensors for easier manipulation\n",
    "misclassified_images = torch.stack(misclassified_images)\n",
    "misclassified_labels = torch.tensor(misclassified_labels)\n",
    "predicted_labels = torch.tensor(predicted_labels)\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "for i in range(len(misclassified_images)):\n",
    "    ax = fig.add_subplot(4, 5, i+1)  # Assuming a maximum of 20 misclassifications\n",
    "    img = misclassified_images[i].permute(1, 2, 0).numpy()  # convert to numpy and rearrange dimensions\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"True: {misclassified_labels[i]}, Predicted: {predicted_labels[i]}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "for i in range(len(correct_images)):\n",
    "    ax = fig.add_subplot(4, 5, i+1)  # Assuming a maximum of 20 misclassifications\n",
    "    img = correct_images[i].permute(1, 2, 0).numpy()  # convert to numpy and rearrange dimensions\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"True: {correct_labels[i]}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our CNN has demonstrated a remarkable improvement over the previous standard neural network model. By incorporating convolutional and pooling layers, the model is now effectively using the spatial information in the images. This is reflected in the improved accuracy of our model on the test images. It's important to note, however, that our dataset is quite small, with only 100 images. While this is enough for us to see the benefits of using a CNN for image classification, in real-world applications we'd likely need thousands (if not millions) of images to achieve high accuracy.\n",
    "\n",
    "Nonetheless, our results are promising. With more images, and potentially more complex and deeper architectures (more layers), we could further improve the accuracy. Also, techniques such as image augmentation (random rotations, scaling, flips, etc.), batch normalization, and different optimization algorithms could also help. This exercise has provided a good foundation to build upon. Deep Learning and CNNs are vast topics with many advanced techniques and models to explore, such as transfer learning and architectures like ResNet, Inception, and more. The sky is the limit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Neural Networks ‚≠Æ\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a type of neural network designed to handle sequential data. Sequential data, or time-series data, is any kind of data where the order matters. This can include sentences (where the order of words is important), stock prices, weather data, etc.\n",
    "\n",
    "Traditional neural networks aren't well suited to these tasks because they process each input independently. If you feed in a sentence one word at a time, a standard neural network will treat each word as a separate entity and won't remember its context in the sentence.\n",
    "\n",
    "RNNs, on the other hand, have loops that allow information to be passed from one step in the sequence to the next. This gives them a kind of memory about what they have processed so far.\n",
    "\n",
    "![](https://i.stack.imgur.com/WSOie.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Despite their advantages, RNNs have their problems. They can be difficult to train effectively due to issues such as vanishing gradients, where the weights in the earlier layers of the network are updated very little during training, making the network difficult to train. More advanced variants of RNNs, such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks, have mechanisms to combat these problems and are commonly used in practice.\n",
    "\n",
    "Let's look at an example of how an RNN can be used for our previous task of predicting the origin of surnames. Note that we stick to simple layers here, but PyTorch has [implementations](https://pytorch.org/docs/stable/nn.html#recurrent-layers) for most relevant recurrent layers out there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Convert character to dense vector - turn our character indices into dense vectors of fixed size\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=n_layers, batch_first=True)  # RNN layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # Fully connected layer for final output\n",
    "        \n",
    "    def forward(self, surname):\n",
    "        embedded = self.embedding(surname)  # First pass characters through embedding layer\n",
    "        output, hidden = self.rnn(embedded)  # Pass embeddings through RNN\n",
    "        final_output = self.fc(hidden.squeeze(0))  # Pass final hidden state through fully connected layer\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(surnames['country'])\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(surnames['surnames'], labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting surnames into ASCII values\n",
    "X_train = [torch.tensor([ord(c) for c in name], dtype=torch.long) for name in X_train]\n",
    "X_test = [torch.tensor([ord(c) for c in name], dtype=torch.long) for name in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we prepare sequences of data for a recurrent neural network (RNN), every sequence within a single batch needs to be of the same length so it can be properly processed by the model in parallel. However, when dealing with real-world sequence data like text or time series, our sequences can often vary in length.\n",
    "\n",
    "For example, in the case of the surnames, different names can have different numbers of characters. To handle this, we can pad the shorter sequences with some special padding symbol (like a zero vector) to make all sequences within a batch the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Padding sequences and creating DataLoader\n",
    "def collate_fn(batch):\n",
    "    surnames, labels = zip(*batch)\n",
    "    surnames_pad = pad_sequence(surnames, batch_first=True)\n",
    "    return surnames_pad, torch.tensor(labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "train_data = list(zip(X_train, y_train))\n",
    "test_data = list(zip(X_test, y_test))\n",
    "\n",
    "train_dl = DataLoader(train_data, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "test_dl = DataLoader(test_data, batch_size=16, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 255  # ASCII range\n",
    "embedding_dim = 64  \n",
    "hidden_dim = 128  \n",
    "output_dim = len(le.classes_) \n",
    "n_layers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neat trick we will introduce now addresses the issue of class imbalance by passing weights to the loss function proportional to the class share in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor((1 / np.bincount(labels)) * 100, dtype=torch.float)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, optimizer and loss function\n",
    "model = SurnameRNN(input_dim, embedding_dim, hidden_dim, output_dim, n_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):  # example number of epochs\n",
    "    for i, (surnamesb, labels) in enumerate(train_dl):\n",
    "        output = model(surnamesb)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predicted = []\n",
    "\n",
    "    for surnamesb, labels in test_dl:\n",
    "        outputs = model(surnamesb)\n",
    "        \n",
    "        # Get predictions from the maximum value\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Total number of labels\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Total correct predictions\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        all_predicted += predicted.tolist()\n",
    "\n",
    "    print(f'Accuracy: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a confusion matrix to visualise how well our model performed for each nationality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, all_predicted)\n",
    "plt.figure(figsize=(4,4))\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our gains in performance using a Recurrent Neural Network (RNN) are modest, if at all, over using a simpler model based on bicharacters and a Multilayer Perceptron (MLP). However, it's important to note that RNNs tend to shine in more complex tasks that involve longer sequences and where the order of elements carries significant information. For instance, in natural language processing tasks like text generation, machine translation, or sentiment analysis of sentences, RNNs typically outperform simpler models because they are able to capture the sequential nature of language and the contextual dependencies between words. Thus, the choice of model often depends on the specific task at hand, its complexity, and the characteristics of the data.\n",
    "\n",
    "In this notebook we won't cover topics like sequence-to-sequence models (Seq2Seq) and Attention Mechanisms, which are important constructs in more advanced models for tasks such as machine translation, text summarization, and even in modern transformer models like GPT and BERT. If you're interested in learning more about these topics, we highly recommend checking out the following resources:\n",
    "- [PyTorch's Tutorial on Seq2Seq translation](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
    "- [Benjamin Warner's annotated implementation of the three main flavours of Attention](https://benjaminwarner.dev/2023/07/01/attention-mechanism)\n",
    "\n",
    "Remember that understanding these more complex models often requires a solid foundation in the basics, so take the time to fully understand RNNs before diving into these advanced topics. Happy learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quick dive on Transformers ü§ñ using Hugging Face ü§ó\n",
    "\n",
    "After our journey through the main architectures in Deep Learning, it's worth mentioning one of the most significant advancements in the field of Natural Language Processing: The Transformer model. This model, introduced in the paper \"Attention is All You Need\" by Vaswani et al., has formed the basis for state-of-the-art models like BERT, GPT, and others.\n",
    "\n",
    "Transformers tackle some of the limitations of RNNs, including long training times and difficulty handling long-range dependencies in the text. They introduced the concept of \"attention\" that allows the model to focus on different parts of the input sequence when producing the output, enabling it to be more context-aware.\n",
    "\n",
    "In the context of deep learning tools, Hugging Face's transformers library stands out as one of the most powerful and popular choices for working with Transformer models. It provides thousands of pre-trained models that can be used out of the box for tasks like text classification, text generation, translation, summarization, and more. The library is also designed to be highly interoperable with PyTorch and TensorFlow, making it a versatile tool for both research and production.\n",
    "\n",
    "A transformer model can be fully implemented in ten or less lines of code for a text classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs, labels=labels)\n",
    "\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In just a few lines of code, we've loaded a pre-trained BERT model and used it to make predictions on a piece of text, even if the task was irrelevant. The transformers library handles all the necessary preprocessing, tokenization, and model inference steps under the hood, allowing us to focus on developing our application.\n",
    "\n",
    "As an illustrative example of a transfer learning task, let's fully spell out a pipeline that combines what we've learnt to predict main concept labels for articles published by faculty members of TalTech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Open Access articles from TalTech\n",
    "articles = getter.get_oa_articles(institution='TT').explode(column=\"concepts\")[[\"abstract\", \"concepts\"]].dropna().reset_index(drop=True)\n",
    "\n",
    "# Expand the 'concepts' column and concatenate it back to the dataframe, keeping only 'abstract' and 'display_name'\n",
    "articles = pd.concat([articles.drop([\"concepts\"], axis=1), pd.json_normalize(articles[\"concepts\"])], axis=1)[[\"abstract\", \"display_name\"]]\n",
    "\n",
    "# Keep articles with display_name in the top 5 most frequent\n",
    "articles = articles[articles[\"display_name\"].isin(articles[\"display_name\"].value_counts().head(5).index)].sample(1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in his case, we are dealing with a multi-label classification problem. This is because each of our articles can have more than one 'concept' associated with it, and our model needs to predict all relevant 'concepts' given an 'abstract'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a dictionary where keys are abstracts and values are lists of associated concepts. Let's also create a custom Dataset object, that allows us to override some of the method functions and adjust them to our multi-label case using transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractConceptDataset(Dataset):\n",
    "    def __init__(self, abstracts, labels, tokenizer, max_length):\n",
    "        self.abstracts = abstracts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.abstracts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        abstract = self.abstracts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            abstract,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "            'abstract': abstract,\n",
    "            'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long).flatten(),  # convert the list to a tensor\n",
    "            'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long).flatten(),  # convert the list to a tensor\n",
    "            'labels': torch.tensor(label, dtype=torch.float),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's create a dictionary where keys are abstracts and values are lists of associated concepts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "abstract_to_concept = articles.groupby('abstract')['display_name'].apply(list).to_dict()\n",
    "\n",
    "# Convert labels to binary vectors\n",
    "mlb = MultiLabelBinarizer()\n",
    "encoded_labels = mlb.fit_transform(list(abstract_to_concept.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "abstracts_train, abstracts_val, labels_train, labels_val = train_test_split(\n",
    "    list(abstract_to_concept.keys()), \n",
    "    encoded_labels, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abstracts_train[:2], \"\\n\\n\", labels_train[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create our datasets and dataloaders. Note that this is not strictly necessary, but simpler approaches require using modules from Hugging Face. You can see an example in [this tutorial](https://www.philschmid.de/getting-started-pytorch-2-0-transformers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AbstractConceptDataset(abstracts_train, labels_train, tokenizer, max_length=512)\n",
    "val_dataset = AbstractConceptDataset(abstracts_val, labels_val, tokenizer, max_length=512)\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dl = DataLoader(val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the model. Note that we could in practice build it in a single line using existing modules:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, let's explicitly define it, and freeze the BERT layer parameters (as otherwise these will take forever to train!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "class BERTMultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_labels):\n",
    "        super(BERTMultiLabelClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Freeze BERT weights\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state_cls = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "# Initialize the model\n",
    "model = BERTMultiLabelClassifier('bert-base-uncased', len(mlb.classes_))\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's train away! We're in fact only training the top classifier layer, so this should be fast. In practice, you can use a few Linear layers with activation functions if you think the model can benefit from the non-linearities, and even fine-tune some of the layers of the transformer if there's a reason to do so and you have the GPU to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, data in enumerate(train_dl):\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1} | Loss: {total_loss/len(train_dl)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rvo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
