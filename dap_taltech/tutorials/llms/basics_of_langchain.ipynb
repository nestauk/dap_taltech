{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëã Basics of LangChain \n",
    "\n",
    "The **key goal** of this tutorial is to get comfortable using the Python library, LangChain. \n",
    "\n",
    "Where you see a **üõ∏ TASK** in the tutorial, you will need to complete the task before moving on.\n",
    "\n",
    "This tutorial relies heavily on a number of different resources mentioned below. Feel free to refer to some of these materials during the tutorial and in your project work: \n",
    "\n",
    "#### Documentation \n",
    "\n",
    "- [langchain](https://python.langchain.com/docs/get_started/introduction.html)\n",
    "    -  [langchain integrations](https://python.langchain.com/docs/integrations): \n",
    "\n",
    "#### Repos \n",
    "\n",
    "- [The Practical Guides to Large Language Models](https://github.com/Mooler0410/LLMsPracticalGuide)\n",
    "- [A series of langchain tutorials](https://github.com/gkamradt/langchain-tutorials)\n",
    "\n",
    "#### Videos\n",
    "\n",
    "- [LangChain Crash Course for Beginners](https://www.youtube.com/watch?v=nAmC7SoVLd8)\n",
    "\n",
    "#### Blogs\n",
    "\n",
    "- [Retrival Augmented Generation](https://betterprogramming.pub/harnessing-retrieval-augmented-generation-with-langchain-2eae65926e82)\n",
    "\n",
    "**NOTE:** We won't be able to examine all of LangChain's capabilities - we are simply exploring its core principals. Please do refer to the additional material mentioned above and the library's documentation to explore its additional functionalities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from dap_taltech.utils.data_getters import DataGetter\n",
    "from dap_taltech import logger\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI, OpenAIChat\n",
    "\n",
    "from langchain.output_parsers import (\n",
    "    PydanticOutputParser, \n",
    "    CommaSeparatedListOutputParser,\n",
    "    DatetimeOutputParser,\n",
    "    EnumOutputParser,\n",
    "    OutputFixingParser,\n",
    "    RetryWithErrorOutputParser)\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "from langchain.chains import (ConversationalRetrievalChain, \n",
    "                              SequentialChain, \n",
    "                              LLMChain)\n",
    "\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts import MessagesPlaceholder, ChatPromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "from langchain.memory import ConversationTokenBufferMemory, ConversationBufferMemory\n",
    "\n",
    "from langchain.document_loaders import WikipediaLoader, DataFrameLoader \n",
    "\n",
    "from langchain.agents import create_pandas_dataframe_agent, tool, OpenAIFunctionsAgent, AgentExecutor\n",
    "from langchain.agents.agent_types import AgentType"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble\n",
    "\n",
    "The code block below is the preamble to this tutorial so that:\n",
    "\n",
    "- we install our dependencies;\n",
    "- have access to our datasets; and \n",
    "- our OpenAI API key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\n",
    "    f\"pip install -r {Path.cwd()}/llm_requirements.txt --quiet\" #install requirements to run this notebook\n",
    ")\n",
    "\n",
    "load_dotenv() # load environment variables\n",
    "\n",
    "oa_key = os.environ.get('OPENAI_API_KEY') #get our open api key from our environment variable \n",
    "\n",
    "if not oa_key:\n",
    "    logger.error(\"No open api key found. Please set your openAI api key as an environment variable named OPENAI_API_KEY.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tour of LangChain ü¶úüîó: Building a language model application\n",
    "\n",
    "LangChain is a python \"framework for developing applications powered by language models.\" \n",
    "\n",
    "According to [LangChain's documentation](https://github.com/langchain-ai/langchain), there are **6 key areas** that it is designed to help you with (in order of complexity):\n",
    "\n",
    "**üìÉ LLMs and Prompts:**\n",
    "\n",
    "This includes prompt management, prompt optimization, a generic interface for all LLMs, and common utilities for working with LLMs.\n",
    "\n",
    "**üîó Chains**:\n",
    "\n",
    "Chains go beyond a single LLM call and involve sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\n",
    "\n",
    "**üìö Data Augmented Generation:**\n",
    "\n",
    "Data Augmented Generation involves specific types of chains that first interact with an external data source to fetch data for use in the generation step. Examples include summarization of long pieces of text and question/answering over specific data sources.\n",
    "\n",
    "**ü§ñ Agents:**\n",
    "\n",
    "Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end-to-end agents.\n",
    "\n",
    "**üß† Memory:**\n",
    "\n",
    "Memory refers to persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\n",
    "\n",
    "**üßê Evaluation:**\n",
    "\n",
    "Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we will touch on most of these concepts, this tutorial will go more in depth on:\n",
    "\n",
    "1. üìÉ LLMs and Prompts;\n",
    "2. üîóü§ñ Chains & Agents; and\n",
    "3. üìö Data Augmented Generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Rome, Italy\n",
      "2. Venice, Italy\n",
      "3. Bologna, Italy\n",
      "4. Naples, Italy\n",
      "5. Tuscany, Italy\n"
     ]
    }
   ],
   "source": [
    "model = OpenAI(temperature=0.9) #instantiate our language model. The temperature parameter controls how \"creative\" the model is.\n",
    "\n",
    "#make this innovation mapping related \n",
    "text = \"What are 5 vacation destinations for someone who likes to eat pasta?\" #define our prompt\n",
    "\n",
    "print(model(text)) #print the output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùì Choosing a model \n",
    "\n",
    "LangChain allows you to pick from many different types of language models and use them in your application. Pending access resrictions (i.e. paying for an API) you can pick from a [range of LLMs](https://python.langchain.com/docs/integrations/llms/) or [chat models](https://python.langchain.com/docs/integrations/chat/). \n",
    "\n",
    "It's worth considering things such as:\n",
    "\n",
    "1. **Privacy**. Do you want to use a model that is hosted on a server or do you want to use a model that is hosted locally?  \n",
    "\n",
    "2. **Cost**. Do you want to pay for access to a model or do you want to use a free model?\n",
    "\n",
    "3. **Token window size**. How long will your prompts be? You can use a model that supports a larger token window size if you need to accomodate longer prompts.\n",
    "\n",
    "4. **LLMs vs. chat models**. What type of model do you need? LLMs are good for generating text, while chat models are good for having conversations. As LangChain states: _LLMs in LangChain refer to pure text completion models. The APIs they wrap take a string prompt as input and output a string completion. Meanwhile, chat models are often backed by LLMs but tuned specifically for having conversations. And, crucially, their provider APIs expose a different interface than pure text completion models. Instead of a single string, they take a list of chat messages as input._  \n",
    "\n",
    "In this tutorial, we'll primarily be using OpenAI LLMs and chat models unless otherwise stated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÉ Prompts \n",
    "\n",
    "Prompts refer to the textual input into the model.\n",
    "\n",
    "You can use LangChain to create prompts, such as a simple hardcoded prompt using the PromptTemplate class or passing examples as part of `few-shot prompting`. \n",
    "\n",
    "**üõ∏ TASK**: Can you re-write the `text` prompt to take as input variables:\n",
    "\n",
    "1. The number of vacation destinations;\n",
    "2. The meal type\n",
    "\n",
    "And run the code block below?\n",
    "\n",
    "Refer to the [prompt template documentation](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/) for guidance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define your prompt template to take as input \"number\" and \"meal\" variables.\n",
    "multiple_input_prompt = PromptTemplate(###) \n",
    "    \n",
    "#format your prompt template with the variables you want to use using PromptTemplate\n",
    "\n",
    "prompt = ###\n",
    "\n",
    "## Format the prompt with the PromptTemplate and convert to string \n",
    "input = prompt.format_prompt().to_string()\n",
    "\n",
    "## pass the prompt to the language model\n",
    "\n",
    "model(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üõ∏ TASK**: Can you create a similar prompt using the `ChatPromptTemplate` class instead for a chat model in the cell below? You will need to instantiate a chat model first. What are the differences in the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = OpenAIChat(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "##Define your prompt template to take as input \"number\" and \"meal\" variables.\n",
    "template = ChatPromptTemplate.from_messages([#### ])\n",
    "\n",
    "##Format your prompt template with the variables you want to use using PromptTemplate\n",
    "messages = template.format_messages(\n",
    "    number=#,\n",
    "    meal=#,\n",
    ")\n",
    "\n",
    "##pass the prompt to the chat model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we understand the basics of constructing a prompt and using a prompt template, let's discuss a common prompt engineering technique called few-shot prompting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÉ Prompts: Few-shot prompting\n",
    "\n",
    "Few-shot prompting is a prompt engineering technique to improve the models output. To do so, you need to pass in a list of examples as part of `few-shot prompting`. In LangChain, a few shot prompt template can be constructed from either a set of examples, or from an Example Selector object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt looks like this:\n",
      "-----------------------------------\n",
      "Question: What is the capital of Estonia?\n",
      "The capital of Estonia is: Tallinn\n",
      "-----------------------------------\n",
      "Question: What is the capital of Estonia?\n",
      "The capital of Estonia is: Tallinn\n",
      "\n",
      "Question: What is the capital of France?\n",
      "The capital of France is: Paris\n",
      "\n",
      "Question: What is the capital of Germany?\n",
      "The capital of Germany is: Berlin\n",
      "\n",
      "Question: What is the capital of Australia?\n",
      "\n",
      "the output using few-shot prompting is: \n",
      "The capital of Australia is: Canberra\n",
      "\n",
      "the output not using examples is: \n",
      "\n",
      "Answer: The capital of Australia is Canberra.\n"
     ]
    }
   ],
   "source": [
    "#We first need to create a list of examples to pass to the prompt template\n",
    "\n",
    "examples = [\n",
    "    {\"question\": \"What is the capital of Estonia?\", \"answer\": \"The capital of Estonia is: Tallinn\"},\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"The capital of France is: Paris\"},\n",
    "    {\"question\": \"What is the capital of Germany?\", \"answer\": \"The capital of Germany is: Berlin\"},\n",
    "] \n",
    "\n",
    "#we then create our prompt template \n",
    "example_prompt = PromptTemplate(input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\")\n",
    "\n",
    "print('The prompt looks like this:')\n",
    "print('-----------------------------------')\n",
    "print(example_prompt.format(**examples[0]))\n",
    "print('-----------------------------------')\n",
    "\n",
    "#instantiating our fewshot prompt template\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"Question: {input}\", \n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "input = prompt.format(input=\"What is the capital of Australia?\")\n",
    "#You can see that in the prompt, we have passed the examples as input to the prompt template:\n",
    "print(input)\n",
    "\n",
    "##lets see what our model does with this prompt\n",
    "output_few_shot = model(input)\n",
    "##what about without examples?\n",
    "output_zero_shot = model(\"Question: What is the capital of Australia?\")\n",
    "print('')\n",
    "print(f'the output using few-shot prompting is: {output_few_shot}')\n",
    "print('')\n",
    "print(f'the output not using examples is: {output_zero_shot}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üõ∏ TASK** Reflect on the following questions: How does the input to the model change when you pass in a list of examples as part of `few-shot prompting`? Why might this be useful for applications? \n",
    "\n",
    "Refer to LangChain's [example selector documentation](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors). When would you use an example selector as opposed to hardcoding examples into the prompt? What are the benefits to the different kinds of example selectors LangChain supports?   \n",
    "\n",
    "Can you create a few-shot prompt template for a prompt of your choice using a [length based example selector](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/length_based) in the cell below?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assume that the list of examples is very large and you need a way to select the most relevant examples \n",
    "# using a length based example selector\n",
    "\n",
    "#define your list of examples here\n",
    "examples = [###\n",
    "    \n",
    "#define your example prompt\n",
    "example_prompt =  PromptTemplate(###\n",
    "\n",
    "#define your example selector \n",
    "example_selector = LengthBasedExampleSelector(###\n",
    "                                              \n",
    "#define your prompt using your example selector\n",
    "\n",
    "prompt = FewShotPromptTemplate(###\n",
    "                               \n",
    "#An example with long input, so it selects only one example.\n",
    "long_string = \"this is a veeeerrrrrryyyyyy long stringggggggggg, it coulddddddnt beeeeeeeee longerrrrrrrr\"\n",
    "input = prompt.format(###\n",
    "                 \n",
    "model(input)                     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÉ Prompts: Parsing output \n",
    "\n",
    "Language models output text. However, sometimes you may want to get more structured information from the model's output. LangChain supports this vis-a-vis output parsers. There are two methods to implement output parsers in LangChain:\n",
    "\n",
    "Via: \n",
    "1. A prompt's format instructions;\n",
    "2. Parsing the output directly.\n",
    "\n",
    "LangChain supports a number of [different output parsers](https://python.langchain.com/docs/modules/model_io/output_parsers/), such as:\n",
    "\n",
    "- **List Parser**: Parses LLM output to be a list of strings.\n",
    "- **JSON Parser**: Parses LLM output to be in JSON format.\n",
    "- **Datetime Parser**: Parses LLM output into datetime format.\n",
    "\n",
    "There are also external libraries that can help with structuring outputs, like [Kor](https://eyurtsev.github.io/kor/), a wrapper to that allows you to specify a schema of what should be extracted and provide examples (like what we saw in **üíå Prompts: Few-shot prompting**).\n",
    "\n",
    "Let's explore the different types of output parsers and writing your own output parser in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your response should be a list of comma separated values, eg: `foo, bar, baz`\n"
     ]
    }
   ],
   "source": [
    "#Lets consider the same prompt again (\"What are 5 vacation destinations for someone who likes to eat pasta?\")\n",
    "\n",
    "#we would like to return a list of vacation destination strings as output. We can do that using LangChain's output parsers\n",
    "\n",
    "#First, lets define our output parser \n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "#print the format instructions to get a sense of what the output parser expects\n",
    "print(format_instructions)\n",
    "\n",
    "#define your Prompt Template, passing the output parser as an argument\n",
    "prompt = PromptTemplate(input_variables=[\"number\", \"meal\"], \n",
    "                        #Add format instructions in the template\n",
    "                        template=\"What are {number} vacation destinations for someone who likes to eat {meal}?\\n{format_instructions}\",\n",
    "                        #we add our format instructions to the partial variable here\n",
    "                        partial_variables={\"format_instructions\": format_instructions})\n",
    "\n",
    "#define your input \n",
    "_input = prompt.format(number=\"3\", meal=\"salad\")\n",
    "\n",
    "#pass the input to the model\n",
    "output = model(_input)\n",
    "output_parsed = output_parser.parse(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets see what the raw output looks like and what the parsed output looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model returns the following output: \n",
      "\n",
      "Rome, Italy, Paris, France, Bangkok, Thailand\n",
      "the parsed output is: ['Rome', 'Italy', 'Paris', 'France', 'Bangkok', 'Thailand']\n"
     ]
    }
   ],
   "source": [
    "print(f\"the model returns the following output: {output}\")\n",
    "print(f\"the parsed output is: {output_parsed}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try a more complex example by defining our own response schema. You can define a response schema for which you want to return multiple fields using the `ResponseSchema` and `StructuredOutputParser` classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the initial output of the model is: \n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"name\": \"Kylie Jenner\",\n",
      "\t\"date\": \"Thursday\"\n",
      "}\n",
      "```\n",
      "the parsed output is: {'name': 'Kylie Jenner', 'date': 'Thursday'}\n"
     ]
    }
   ],
   "source": [
    "schema = [ResponseSchema(name=\"name\", \n",
    "                         description=\"The first and last name of a person. Must be a string.\"),\n",
    "          ResponseSchema(name=\"date\", \n",
    "                         description=\"The date of an event. Must be a string.\")]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(schema)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "#define your Prompt Template, passing the output parser as an argument\n",
    "prompt = PromptTemplate(input_variables=[\"sentence\"], \n",
    "                        #Add format instructions in the template\n",
    "                        template=\"Extract names and dates from the following sentence:{sentence}\\n{format_instructions}\",\n",
    "                        #we add our format instructions to the partial variable here\n",
    "                        partial_variables={\"format_instructions\": format_instructions})\n",
    "\n",
    "#define your input \n",
    "sentence = \"Kylie Jenner turned 26-years-old on Thursday.\"\n",
    "_input = prompt.format(sentence=sentence)\n",
    "\n",
    "#pass the input to the model\n",
    "output = model(_input)\n",
    "output_parsed = output_parser.parse(output)\n",
    "\n",
    "print(f\"the initial output of the model is: {output}\")\n",
    "print(f\"the parsed output is: {output_parsed}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üõ∏ TASK**: Experiment with different prompts and types of output parsers that LangChain supports. What are the purposes of `OutputFixingParser` and `RetryWithErrorOutputParser`? Can you write your own custom output parser?\n",
    "\n",
    "Refer to documentation on [output parsers](https://python.langchain.com/docs/modules/model_io/output_parsers/) for guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÉ Prompts: Chaining it all together\n",
    "\n",
    "So far, we've learned about:\n",
    "\n",
    "1. Writing a simple prompt;\n",
    "2. Using LangChain's prompt templates;\n",
    "3. Using few-shot prompting and;\n",
    "4. Parsing the output of the model by passing formatting instructions. \n",
    "\n",
    "This should give you a good foundation to start building your own prompts and is a good departure point to explore LangChain's **üîóü§ñ chains & agents.** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîóü§ñ Chains & Agents\n",
    "\n",
    "The core idea of LangChain is that we can \"chain\" together different components to create more advanced use cases around LLMs. Chains are simply end-to-end wrappers around multiple individual components.\n",
    "\n",
    "The simplest chain is the LLMchain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. We've already explored all 3 of these components in the previous sections.  \n",
    "\n",
    "Let's see what that looks like in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Kim Kardashian', 'text': '\\n\\nKendall Jenner.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define your prompt\n",
    "prompt_template = \"What is a good alternative name for a celebrity called {name}?\"\n",
    "\n",
    "#here we define the chain - we pass the prompt template and the model to the LLMChain\n",
    "llm_chain = LLMChain(\n",
    "    llm=model,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")\n",
    "\n",
    "#execute your chain\n",
    "llm_chain('Kim Kardashian')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also pass an output parser to the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adjective': 'spicy',\n",
       " 'meal': 'laksa',\n",
       " 'text': '\\n\\n-Rice vermicelli\\n-Curry paste (of your choice)\\n-Canned coconut milk\\n-Vegetable stock\\n-Shallots\\n-Garlic\\n-Shrimps\\n-Fish sauce\\n-Sugar\\n-Lemongrass\\n-Lime juice\\n-Coriander\\n-Cumin\\n-Red chilli\\n-Red onion\\n-Tomatoes\\n-Fresh coriander'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instantiate the output parser\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "#define your prompt\n",
    "template = \"\"\"Give me the ingredients for a {adjective} {meal}\"\"\"\n",
    "\n",
    "#instantiate the prompt template\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"adjective\", \"meal\"], output_parser=output_parser)\n",
    "\n",
    "#chain the prompt template and the model\n",
    "llm_chain = LLMChain(prompt=prompt, \n",
    "                     llm=model)\n",
    "\n",
    "#execute your chain\n",
    "llm_chain({'adjective': \"spicy\", 'meal': \"laksa\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîóü§ñ Chains & Agents: Sequential and Index-related chains"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets explore a few other chains and categories of chains that LangChain supports: \n",
    "\n",
    "1. `SequentialChain`: A chain that executes a sequence of chains in order.\n",
    "2. **Index-related chains**: A category of chains that allows you to combine your own data (stored in indexes) with LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an LLMChain to write a headline for a celebrity gossip article\n",
    "\n",
    "llm = OpenAI(temperature=.7)\n",
    "template = \"\"\"You are a news reporter. Given a celebrity name and a date, write a gossip headline for an article about them.\n",
    "\n",
    "name: {name}\n",
    "date: {date}\n",
    "\n",
    "news reporter: This just in:\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(input_variables=[\"name\", \"date\"], template=template)\n",
    "gossip_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=\"gossip\")\n",
    "\n",
    "# This is an LLMChain to write a celebrity response, given the headline. \n",
    "\n",
    "llm = OpenAI(temperature=.7)\n",
    "\n",
    "template = \"\"\"You are a celebrity. Given a headline about you, you are writing a response to it.\n",
    "\n",
    "News headline:\n",
    "{gossip}\n",
    "\n",
    "Response from a celebrity of the above headline:\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(input_variables=[\"gossip\"], template=template)\n",
    "response_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=\"response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'Drake',\n",
       " 'date': 'April 2021',\n",
       " 'gossip': ' Drake Sparks Romance Rumors in April 2021!',\n",
       " 'response': \" \\nI'm flattered by the rumors, but I'm focused on my music right now. I'm just enjoying the moment.\"}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is the overall chain where we chain the two chains together\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[gossip_chain, response_chain],\n",
    "    input_variables=[\"name\", \"date\"],\n",
    "    # Here we return multiple variables\n",
    "    output_variables=[\"gossip\", \"response\"],\n",
    "    verbose=True)\n",
    "\n",
    "overall_chain({\"name\":\"Drake\", \"date\": \"April 2021\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at index-related chains. This category of chains are used for interacting with indexes, with the key purpose of allowing you to combine your own data (stored in indexes) with LLMs. \n",
    "\n",
    "There are a few different types of index-related chains: `stuff`, `map_reduce` and `refine`.\n",
    "\n",
    "`stuff`: you simply stuff all the related data into the prompt as context to pass to the language model.\n",
    "\n",
    "`map_reduce`: This method involves running an initial prompt on each chunk of data (for summarization tasks, this could be a summary of that chunk; for question-answering tasks, it could be an answer based solely on that chunk). Then a different prompt is run to combine all the initial outputs. \n",
    "\n",
    "`refine`: This method involves running an initial prompt on the first chunk of data, generating some output. For the remaining documents, that output is passed in, along with the next document, asking the LLM to refine the output based on the new document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94;1;1m2023-08-11 15:17:18,293 - TalTech HackWeek 2023 - INFO - Loading data from open dap-taltech s3 bucket. (data_getters.py:58)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output_text': ' Types of inventions patented include a method of making a transparent visible light activated photocatalytic superhydrophilic glass material, a system and method for power transfer between two DC voltage sources, a therapeutic mud mixture, and a sensor for the detection of Neurotrophic Factor.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First, lets load a pre-defined question answering chain\n",
    "\n",
    "#lets load some data\n",
    "dg = DataGetter(local=False)\n",
    "#we're going to get a sample of 10 estonian patents\n",
    "#who's assignee is from tallinn\n",
    "patents_sample = (dg.get_estonian_patents()\n",
    " .explode('assignee_harmonized_names')\n",
    " .query('assignee_harmonized_names.str.contains(\"TALLINN\")')\n",
    " .drop_duplicates('family_id')\n",
    " .sample(10, random_state=42))\n",
    "\n",
    "#lets load the data into a dataframe loader\n",
    "loader = DataFrameLoader(patents_sample, page_content_column=\"abstract_localized\")\n",
    "docs =loader.load()\n",
    "\n",
    "#lets define our prompt - we want to ask questions about patents\n",
    "#from tallinn assignees using patent abstracts \n",
    "prompt = \"\"\"You are a patent examiner. Given patent abstracts, answer the following question:\n",
    "\n",
    "{Question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "chain = load_qa_chain(llm, chain_type=\"map_reduce\") #here, we can define the chain type to be \"stuff\", \"map_reduce\" or \"refine\".\n",
    "chain({\"input_documents\": docs, \n",
    "       \"question\": \"What are the types of inventions patented?\"}, return_only_outputs=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üõ∏ TASK**: Spend some time exploring the types of chains that LangChain supports. Can you build your own chain using a combination of the components we've explored so far?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can investigate the different types of chains by\n",
    "#calling help on langchain.chains\n",
    "\n",
    "#help(langchain.chains)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîóü§ñ Chains & Agents: Agents\n",
    "\n",
    "Now that we've explored a few different types of chains that LangChain supports, let's pivot to exploring agents.\n",
    "\n",
    "Chains and agents are somewhat similar. However, in chains, a sequence of actions is hardcoded (in code). In agents, a language model is used as a reasoning engine to determine which actions to take and in which order. Here are a list of [Agent types that langchain supports](https://python.langchain.com/docs/modules/agents/agent_types/). \n",
    "\n",
    "Key to an agent are tools. Tools are functions that an agent calls. You can define your own tools by adding a `tool` decorator to a function or use any of the pre-defined tools that LangChain supports.\n",
    "\n",
    "Let's walk through a simple example first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_s_count` with `{'word': 'Sesquipedalian'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m2\u001b[0m\u001b[32;1m\u001b[1;3mThe letter 's' appears 2 times in the word \"Sesquipedalian\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The letter \\'s\\' appears 2 times in the word \"Sesquipedalian\".'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets load our chat model \n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "#lets define a really simple tool \n",
    "#to return the number of s's in a word\n",
    "@tool\n",
    "def get_s_count(word: str) -> int:\n",
    "    \"\"\"Returns a count of the number of s's in a word\"\"\"\n",
    "    return word.lower().count(\"s\")\n",
    "\n",
    "system_message = SystemMessage(content=\"You are very powerful assistant, but bad at calculating the number of times the letter s appears in a word.\")\n",
    "prompt = OpenAIFunctionsAgent.create_prompt(system_message=system_message)\n",
    "\n",
    "tools = [get_s_count]\n",
    "#putting it alltogether\n",
    "agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt)\n",
    "#this defines the run time for the agent\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.run(\"how many times do you spot the letter 's' in the word Sesquipedalian?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila - we have an agent! However, the agent is stateless - meaning it doesn't remember anything about previous interactions, making follow up questions difficult. I don't know about you, but I have no idea what \"Sesquipedalian\" means. Let's add memory to fix this and ask a few follow up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_s_count` with `{'word': 'Sesquipedalian'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m2\u001b[0m\u001b[32;1m\u001b[1;3mThe letter 's' appears 2 times in the word \"Sesquipedalian\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\"Sesquipedalian\" is an adjective that means using long words or characterized by long words; long-winded. It is often used to describe someone who tends to use excessively long and complex words in their speech or writing.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mCertainly! Here's an example sentence using the word \"sesquipedalian\":\n",
      "\n",
      "\"During the lecture, the professor's sesquipedalian style of speaking made it difficult for the students to understand the concepts.\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Certainly! Here\\'s an example sentence using the word \"sesquipedalian\":\\n\\n\"During the lecture, the professor\\'s sesquipedalian style of speaking made it difficult for the students to understand the concepts.\"'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MEMORY_KEY = \"chat_history\"\n",
    "prompt = OpenAIFunctionsAgent.create_prompt(\n",
    "    system_message=system_message,\n",
    "    extra_prompt_messages=[MessagesPlaceholder(variable_name=MEMORY_KEY)]\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=MEMORY_KEY, return_messages=True)\n",
    "\n",
    "agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True)\n",
    "agent_executor.run(\"how many times do you spot the letter 's' in the word Sesquipedalian?\")\n",
    "agent_executor.run(\"what does that word even mean?\")\n",
    "agent_executor.run(\"can you use it in a sentence?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great - we've created a simple agent that can remember previous interactions. \n",
    "\n",
    "Let's explore different agents by re-visiting our chain that summarised a series of estonian patents as a use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to look at the title_localized column\n",
      "Action: python_repl_ast\n",
      "Action Input: df['title_localized'].tolist()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['A method of making a transparent visual light activated photocatalytic superhydrophilic glass material', 'Method and device for measuring charcteristics of refelection of light on surfaces', 'Method of shoot-through generation for modified sine wave z-source, quasi-z-source and trans-z-source inverters', 'Method of making a portable mip-based electrochemical sensor for the detection of the sars-cov-2 antigen', 'System and method for a partial power transfer between two dc sources', 'Synthesis and polymerization of isosorbide-based monomethacrylates', 'Therapeutic mud mixture and a method for its manufacture', 'Method and device for measuring and monitoring concentration of substances in a biological fluid', 'Method and device for frequency response measurement', 'Molecularly imprinted polymer based electrically conductive surface, method of its preparation and sensor for neurotrophic factors']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The patents are about a transparent visual light activated photocatalytic superhydrophilic glass material, measuring characteristics of reflection of light on surfaces, shoot-through generation for modified sine wave z-source, quasi-z-source and trans-z-source inverters, a portable mip-based electrochemical sensor for the detection of the sars-cov-2 antigen, a partial power transfer between two dc sources, synthesis and polymerization of isosorbide-based monomethacrylates, a therapeutic mud mixture and a method for its manufacture, measuring and monitoring concentration of substances in a biological fluid, frequency response measurement, and a molecularly imprinted polymer based electrically conductive surface, method of its preparation and sensor for neurotrophic factors.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to look at the dataframe\n",
      "Action: python_repl_ast\n",
      "Action Input: df.info()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10 entries, 889 to 958\n",
      "Data columns (total 10 columns):\n",
      " #   Column                     Non-Null Count  Dtype         \n",
      "---  ------                     --------------  -----         \n",
      " 0   publication_number         10 non-null     object        \n",
      " 1   family_id                  10 non-null     object        \n",
      " 2   title_localized            10 non-null     object        \n",
      " 3   publication_date           10 non-null     datetime64[ns]\n",
      " 4   grant_date                 0 non-null      datetime64[ns]\n",
      " 5   cpc                        10 non-null     object        \n",
      " 6   inventor_harmonized        10 non-null     object        \n",
      " 7   assignee_harmonized        10 non-null     object        \n",
      " 8   inventor_harmonized_names  10 non-null     object        \n",
      " 9   assignee_harmonized_names  10 non-null     object        \n",
      "dtypes: datetime64[ns](2), object(8)\n",
      "memory usage: 880.0+ bytes\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The dataframe contains 10 entries, with 10 columns of data. The columns contain information about the patent, such as the publication number, family id, title, publication date, grant date, cpc, inventor and assignee harmonized names, and inventor and assignee harmonized names.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The dataframe contains 10 entries, with 10 columns of data. The columns contain information about the patent, such as the publication number, family id, title, publication date, grant date, cpc, inventor and assignee harmonized names, and inventor and assignee harmonized names.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets create a dataframe agent \n",
    "\n",
    "#lets get our patents data\n",
    "patents_sample = (dg.get_estonian_patents()\n",
    " .explode('assignee_harmonized_names')\n",
    " .query('assignee_harmonized_names.str.contains(\"TALLINN\")')\n",
    " .drop_duplicates('family_id')\n",
    " .sample(10, random_state=42)\n",
    " .drop(columns=['inventor_harmonized_country_codes', 'assignee_harmonized_country_codes', 'abstract_localized', 'country_code', 'application_number', 'filing_date', 'priority_date']))\n",
    "\n",
    "#lets add memory so we can ask follow up questions\n",
    "MEMORY_KEY = \"chat_history\"\n",
    "prompt = OpenAIFunctionsAgent.create_prompt(\n",
    "    system_message=system_message,\n",
    "    extra_prompt_messages=[MessagesPlaceholder(variable_name=MEMORY_KEY)]\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=MEMORY_KEY, return_messages=True)\n",
    "\n",
    "#let's instantiate a pandas df agent with a chat model                    \n",
    "agent = create_pandas_dataframe_agent(OpenAI(temperature=0), patents_sample, verbose=True, memory=memory)\n",
    "agent.run(\"what are the patents about?\")\n",
    "agent.run(\"can you tell me more about the patents?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üõ∏ TASK**: Reflect on the chain vs. agent approach. How do the two differ? \n",
    "\n",
    "Build your own **agent** using tools [from a list of available langchain tools](https://python.langchain.com/docs/integrations/tools/) and a combination of the components we've explored so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build your own agent here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîóü§ñ Chains & Agents: Chaining it all together\n",
    "\n",
    "Nice. Now we're familiar with prompts, chains and agents. We also dabbled in using our own data (primarily patents) as part of building our LLM application. \n",
    "\n",
    "LangChain provides much more functionality for using external data sources than what we've seen. Using external data is a key part to building real-world use cases with LLM applications. Let's explore this in more detail in the cells below. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Data Augmented Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö Data Augmented Generation: Document Loaders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've been using data from our utils library. LangChain has many different integrations to be able to load data to use external sources in your LLM application, ranging from loading your own data from a local directory or AWS's S3 to external data sources from Twitter or Open City Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/india.kerlenesta/opt/anaconda3/envs/dap_taltech/lib/python3.9/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/india.kerlenesta/opt/anaconda3/envs/dap_taltech/lib/python3.9/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We have loaded a list of Document objects related to the Wikipedia query \"Barbie\"\n",
    "barbie_pages = WikipediaLoader(query=\"Barbie\", load_max_docs=10).load()\n",
    "len(barbie_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The page content begins as follows:\n",
      "\n",
      "Barbenheimer is an Internet phenomenon that began on social media before the simultaneous theatrical release of two blockbuster films, Barbie and Oppenheimer, on July 21, 2023, in the United States and several other countries. The word is a portmanteau of the films' titles. The contrast of Barbie‚Äîa fantasy comedy by Greta Gerwig about the fashion doll Barbie‚Äîand Oppenheimer‚Äîan epic biographical thriller by Christopher Nolan about physicist J. Robert Oppenheimer, scientific director of the Manhattan Project, which developed the first nuclear weapons during World War II‚Äîprompted a comedic response from Internet users, including memes and merchandise. Polygon described the two films as \"extreme opposites\", and Variety called the phenomenon \"the movie event of the year\".The films' simultaneous release was an instance of counterprogramming. As their release date approached, instead of generating a rivalry, suggestions emerged to watch the films as a double feature‚Äîas well as what order to w...\n"
     ]
    }
   ],
   "source": [
    "#lets have a look at a Document\n",
    "print(f\"The page content begins as follows:\")\n",
    "print('') \n",
    "print(f\"{barbie_pages[3].page_content[:1000]}...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's as easy as that! Feel free to refer to LangChain's document loaders [here](https://python.langchain.com/docs/integrations/document_loaders/) to explore different types of loaders. \n",
    "\n",
    "We won't explore document loading or transformation too much in this tutorial. Instead, we pivot to focus on combining LLMs and traditional Information Retrieval (IR) techniques called Retrieval Augmented Generation (RAG), using langchain's document loaders as a departure point for building a vector database."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö Data Augmented Generation: Retrival"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've already seen, we can load external data sources and pass them to our LLMs as context via a prompt. However, sometimes when our data is much larger, we want to be able to retrieve the most relevant datapoints first. This is where **Retrieval Augmented Generation (RAG)** comes in.\n",
    "\n",
    "**RAG** is a new generative paradigm that fuses Large Language Models and traditional Information Retrieval (IR) techniques. We can use a retrieval system for the input prompt to augment the output generated by the LLM. This technique allows us to bypass fine-tuning as we can easily expose the model to external data (non-parametric), instead of having to retrain it on our domain-specific data. There are a number of advantages to RAG including:\n",
    "\n",
    "1. **Easy Knowledge Acquisition.** RAG methods allow can easily acquire knowledge from external sources, improving LLM performance within domain specific tasks.   \n",
    "\n",
    "2. **Minimal Training Cost.** The only training needed is the indexing of your knowledge base. No fine-tuning necessary.\n",
    "\n",
    "3. **Multiple Sources of Knowledge.**  With RAG, one can make use of multiple sources of knowledge, including those that are baked into the model parameters as well as information contained within many different knowledge bases.\n",
    "\n",
    "4. **Scalability.** Using performant vector databases, we can easily scale RAG to large datasets and handle complex queries.\n",
    "\n",
    "5. **Improved Performance & Reduced Hallucination.** RAG generates more accurate and contextually informed content by leveraging retrieval techniques, reducing the likelihood of generating incorrect or fabricated information.\n",
    "\n",
    "6. **Overcome Context-Window Limit.** All language models have a fixed length of tokens they can process at once, known as the context-window. Using Retrieval Augmentation, we can overcome this fixed text constraint, allowing the model to incorporate data from larger document collections \n",
    "\n",
    "7. **Return Sources.** RAG also offers explainability, which is essential for building trust in LLMs. Unlike a black-box LLM, RAG allows users to read the sources they retrieved and judge their relevance and credibility for themselves.\n",
    "\n",
    "_Taken from [Harnessing Retrieval Augmented Generation With Langchain](https://betterprogramming.pub/harnessing-retrieval-augmented-generation-with-langchain-2eae65926e82)_\n",
    "\n",
    "Let's build on our knowledge of prompts, chains, agents and document loading to explore RAG in more detail in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we will create a vector database from 9 barbie related wikipedia pages...\n"
     ]
    }
   ],
   "source": [
    "#1. Load data\n",
    "\n",
    "# First, we need to load data from a document loader - let's revisit our barbie example by loading barbie related wikipedia pages\n",
    "print(f\"we will create a vector database from {len(barbie_pages)} barbie related wikipedia pages...\")\n",
    "\n",
    "#2. Preprocess data\n",
    "\n",
    "# As we've already loaded the barbie pages, we need to preprocess the data next. \n",
    "# Let's revisit some of the techniques we learned in the text analysis tutorial to \n",
    "#preprocess our wikipedia pages. Let's chunk and tokenize our documents.\n",
    "\n",
    "##It‚Äôs important to chunk the data as we want to embed a meaningful length of context within our vector index. \n",
    "# Embedding just a word or two is too little information to match relevant vectors, and embedding entire pages would be too long \n",
    "# to fit within the context window of the prompt. Try to strike the right balance for your use case and dataset.\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=25)\n",
    "docs = text_splitter.split_documents(barbie_pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##3. Index your data\n",
    " \n",
    "# Once we‚Äôve gathered our data sources, it‚Äôs time to build our knowledge-base index. \n",
    "# In general, the term ‚Äúindex‚Äù refers to a data structure that is used to optimize the retrieval of information \n",
    "# from a larger collection of data.\n",
    "# In this demo, I'll use the chromadb vector database, a free, open-source vector store that\n",
    "#runs on your local machine and OpenAI embeddings. \n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "#build our vector store with OpenAI embeddings and barbie pages\n",
    "db = Chroma.from_documents(docs, embeddings)\n",
    "\n",
    "##4. Build a Retriever\n",
    "\n",
    "#Once our vector store is indexed, it‚Äôs time to define our retriever. Retriever is the module that determines \n",
    "# how the relevant documents are fetched from the vector database, determined by its search algorithm.\n",
    "# load index\n",
    "\n",
    "# initialize base retriever\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3}) #we will return the top 3 results\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm) #this will add contextual compression, meaning it will\n",
    "#iterate over the initially returned documents \n",
    "# and extract from each only the context relevant to the query, not the whole wikipedia page.\n",
    "reranker = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever) \n",
    "\n",
    "##5. Create our Conversational Retrieval Chain\n",
    "\n",
    "#Now that we've stored our data in a vector store and defined our retriever, \n",
    "# we can create our conversational retrieval chain.\n",
    "\n",
    "\n",
    "#Lets define memory so we can ask follow up questions\n",
    "memory = ConversationTokenBufferMemory(llm=llm, \n",
    "                                       memory_key=\"chat_history\", \n",
    "                                       return_messages=True, \n",
    "                                       input_key='question', \n",
    "                                       max_token_limit=1000)\n",
    "#Let's define our LLM chain\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "question_generator = LLMChain(llm=llm, \n",
    "                              prompt=CONDENSE_QUESTION_PROMPT, \n",
    "                              verbose=True)\n",
    "#Let's define our answer chain\n",
    "answer_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\", verbose=True)\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "            retriever=reranker,\n",
    "            question_generator=question_generator,\n",
    "            combine_docs_chain=answer_chain,\n",
    "            verbose=True,\n",
    "            memory=memory,\n",
    "            rephrase_question=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! Now we can ask all our barbie related questions üíÖ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \n",
      "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
      "ALWAYS return a \"SOURCES\" part in your answer.\n",
      "\n",
      "QUESTION: Which state/country's law governs the interpretation of the contract?\n",
      "=========\n",
      "Content: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an  injunction or other relief to protect its Intellectual Property Rights.\n",
      "Source: 28-pl\n",
      "Content: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)  right or remedy.\n",
      "\n",
      "11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation  in force of the remainder of the term (if any) and this Agreement.\n",
      "\n",
      "11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any  kind between the parties.\n",
      "\n",
      "11.9 No Third-Party Beneficiaries.\n",
      "Source: 30-pl\n",
      "Content: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\n",
      "Source: 4-pl\n",
      "=========\n",
      "FINAL ANSWER: This Agreement is governed by English law.\n",
      "SOURCES: 28-pl\n",
      "\n",
      "QUESTION: What did the president say about Michael Jackson?\n",
      "=========\n",
      "Content: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n",
      "\n",
      "Last year COVID-19 kept us apart. This year we are finally together again. \n",
      "\n",
      "Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n",
      "\n",
      "With a duty to one another to the American people to the Constitution. \n",
      "\n",
      "And with an unwavering resolve that freedom will always triumph over tyranny. \n",
      "\n",
      "Six days ago, Russia‚Äôs Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \n",
      "\n",
      "He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \n",
      "\n",
      "He met the Ukrainian people. \n",
      "\n",
      "From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \n",
      "\n",
      "Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\n",
      "Source: 0-pl\n",
      "Content: And we won‚Äôt stop. \n",
      "\n",
      "We have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \n",
      "\n",
      "Let‚Äôs use this moment to reset. Let‚Äôs stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \n",
      "\n",
      "Let‚Äôs stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \n",
      "\n",
      "We can‚Äôt change how divided we‚Äôve been. But we can change how we move forward‚Äîon COVID-19 and other issues we must face together. \n",
      "\n",
      "I recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \n",
      "\n",
      "They were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \n",
      "\n",
      "Officer Mora was 27 years old. \n",
      "\n",
      "Officer Rivera was 22. \n",
      "\n",
      "Both Dominican Americans who‚Äôd grown up on the same streets they later chose to patrol as police officers. \n",
      "\n",
      "I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\n",
      "Source: 24-pl\n",
      "Content: And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \n",
      "\n",
      "To all Americans, I will be honest with you, as I‚Äôve always promised. A Russian dictator, invading a foreign country, has costs around the world. \n",
      "\n",
      "And I‚Äôm taking robust action to make sure the pain of our sanctions  is targeted at Russia‚Äôs economy. And I will use every tool at our disposal to protect American businesses and consumers. \n",
      "\n",
      "Tonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \n",
      "\n",
      "America will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \n",
      "\n",
      "These steps will help blunt gas prices here at home. And I know the news about what‚Äôs happening can seem alarming. \n",
      "\n",
      "But I want you to know that we are going to be okay.\n",
      "Source: 5-pl\n",
      "Content: More support for patients and families. \n",
      "\n",
      "To get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \n",
      "\n",
      "It‚Äôs based on DARPA‚Äîthe Defense Department project that led to the Internet, GPS, and so much more.  \n",
      "\n",
      "ARPA-H will have a singular purpose‚Äîto drive breakthroughs in cancer, Alzheimer‚Äôs, diabetes, and more. \n",
      "\n",
      "A unity agenda for the nation. \n",
      "\n",
      "We can do this. \n",
      "\n",
      "My fellow Americans‚Äîtonight , we have gathered in a sacred space‚Äîthe citadel of our democracy. \n",
      "\n",
      "In this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \n",
      "\n",
      "We have fought for freedom, expanded liberty, defeated totalitarianism and terror. \n",
      "\n",
      "And built the strongest, freest, and most prosperous nation the world has ever known. \n",
      "\n",
      "Now is the hour. \n",
      "\n",
      "Our moment of responsibility. \n",
      "\n",
      "Our test of resolve and conscience, of history itself. \n",
      "\n",
      "It is in this moment that our character is formed. Our purpose is found. Our future is forged. \n",
      "\n",
      "Well I know this nation.\n",
      "Source: 34-pl\n",
      "=========\n",
      "FINAL ANSWER: The president did not mention Michael Jackson.\n",
      "SOURCES:\n",
      "\n",
      "QUESTION: Who directed the Barbie film?\n",
      "=========\n",
      "Content: Greta Gerwig\n",
      "Source: https://en.wikipedia.org/wiki/Barbie_(film)\n",
      "\n",
      "Content: Mattel partnered with animation studios to produce films which were broadcast on Nickelodeon in the United States from 2002 and released on home video formats, originally by Family Home Entertainment and successor Lionsgate, and then predominantly by Universal Pictures Home Entertainment, both until 2017.\n",
      "Source: https://en.wikipedia.org/wiki/List_of_Barbie_animated_films\n",
      "=========\n",
      "FINAL ANSWER:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The director of the Barbie film is Greta Gerwig.\\nSOURCES: https://en.wikipedia.org/wiki/Barbie_(film), https://en.wikipedia.org/wiki/List_of_Barbie_animated_films'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Who directed the Barbie film?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: Who directed the Barbie film?\n",
      "Assistant: The director of the Barbie film is Greta Gerwig.\n",
      "SOURCES: https://en.wikipedia.org/wiki/Barbie_(film), https://en.wikipedia.org/wiki/List_of_Barbie_animated_films\n",
      "Follow Up Input: Who did she produce the film with?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \n",
      "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
      "ALWAYS return a \"SOURCES\" part in your answer.\n",
      "\n",
      "QUESTION: Which state/country's law governs the interpretation of the contract?\n",
      "=========\n",
      "Content: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an  injunction or other relief to protect its Intellectual Property Rights.\n",
      "Source: 28-pl\n",
      "Content: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)  right or remedy.\n",
      "\n",
      "11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation  in force of the remainder of the term (if any) and this Agreement.\n",
      "\n",
      "11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any  kind between the parties.\n",
      "\n",
      "11.9 No Third-Party Beneficiaries.\n",
      "Source: 30-pl\n",
      "Content: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\n",
      "Source: 4-pl\n",
      "=========\n",
      "FINAL ANSWER: This Agreement is governed by English law.\n",
      "SOURCES: 28-pl\n",
      "\n",
      "QUESTION: What did the president say about Michael Jackson?\n",
      "=========\n",
      "Content: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n",
      "\n",
      "Last year COVID-19 kept us apart. This year we are finally together again. \n",
      "\n",
      "Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n",
      "\n",
      "With a duty to one another to the American people to the Constitution. \n",
      "\n",
      "And with an unwavering resolve that freedom will always triumph over tyranny. \n",
      "\n",
      "Six days ago, Russia‚Äôs Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \n",
      "\n",
      "He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \n",
      "\n",
      "He met the Ukrainian people. \n",
      "\n",
      "From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \n",
      "\n",
      "Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\n",
      "Source: 0-pl\n",
      "Content: And we won‚Äôt stop. \n",
      "\n",
      "We have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \n",
      "\n",
      "Let‚Äôs use this moment to reset. Let‚Äôs stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \n",
      "\n",
      "Let‚Äôs stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \n",
      "\n",
      "We can‚Äôt change how divided we‚Äôve been. But we can change how we move forward‚Äîon COVID-19 and other issues we must face together. \n",
      "\n",
      "I recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \n",
      "\n",
      "They were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \n",
      "\n",
      "Officer Mora was 27 years old. \n",
      "\n",
      "Officer Rivera was 22. \n",
      "\n",
      "Both Dominican Americans who‚Äôd grown up on the same streets they later chose to patrol as police officers. \n",
      "\n",
      "I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\n",
      "Source: 24-pl\n",
      "Content: And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \n",
      "\n",
      "To all Americans, I will be honest with you, as I‚Äôve always promised. A Russian dictator, invading a foreign country, has costs around the world. \n",
      "\n",
      "And I‚Äôm taking robust action to make sure the pain of our sanctions  is targeted at Russia‚Äôs economy. And I will use every tool at our disposal to protect American businesses and consumers. \n",
      "\n",
      "Tonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \n",
      "\n",
      "America will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \n",
      "\n",
      "These steps will help blunt gas prices here at home. And I know the news about what‚Äôs happening can seem alarming. \n",
      "\n",
      "But I want you to know that we are going to be okay.\n",
      "Source: 5-pl\n",
      "Content: More support for patients and families. \n",
      "\n",
      "To get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \n",
      "\n",
      "It‚Äôs based on DARPA‚Äîthe Defense Department project that led to the Internet, GPS, and so much more.  \n",
      "\n",
      "ARPA-H will have a singular purpose‚Äîto drive breakthroughs in cancer, Alzheimer‚Äôs, diabetes, and more. \n",
      "\n",
      "A unity agenda for the nation. \n",
      "\n",
      "We can do this. \n",
      "\n",
      "My fellow Americans‚Äîtonight , we have gathered in a sacred space‚Äîthe citadel of our democracy. \n",
      "\n",
      "In this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \n",
      "\n",
      "We have fought for freedom, expanded liberty, defeated totalitarianism and terror. \n",
      "\n",
      "And built the strongest, freest, and most prosperous nation the world has ever known. \n",
      "\n",
      "Now is the hour. \n",
      "\n",
      "Our moment of responsibility. \n",
      "\n",
      "Our test of resolve and conscience, of history itself. \n",
      "\n",
      "It is in this moment that our character is formed. Our purpose is found. Our future is forged. \n",
      "\n",
      "Well I know this nation.\n",
      "Source: 34-pl\n",
      "=========\n",
      "FINAL ANSWER: The president did not mention Michael Jackson.\n",
      "SOURCES:\n",
      "\n",
      "QUESTION: Who did she produce the film with?\n",
      "=========\n",
      "Content: Mattel partnered with animation studios to produce films.\n",
      "Source: https://en.wikipedia.org/wiki/List_of_Barbie_animated_films\n",
      "\n",
      "Content: Laurence Mark producing.\n",
      "Source: https://en.wikipedia.org/wiki/Barbie_(film)\n",
      "=========\n",
      "FINAL ANSWER:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'She produced the film with Laurence Mark.\\nSOURCES: https://en.wikipedia.org/wiki/Barbie_(film)'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Who did she produce the film with?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: Who directed the Barbie film?\n",
      "Assistant: The director of the Barbie film is Greta Gerwig.\n",
      "SOURCES: https://en.wikipedia.org/wiki/Barbie_(film), https://en.wikipedia.org/wiki/List_of_Barbie_animated_films\n",
      "Human: Who did she produce the film with?\n",
      "Assistant: She produced the film with Laurence Mark.\n",
      "SOURCES: https://en.wikipedia.org/wiki/Barbie_(film)\n",
      "Follow Up Input: What does Barbie have to do with Oppenheimer?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \n",
      "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
      "ALWAYS return a \"SOURCES\" part in your answer.\n",
      "\n",
      "QUESTION: Which state/country's law governs the interpretation of the contract?\n",
      "=========\n",
      "Content: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an  injunction or other relief to protect its Intellectual Property Rights.\n",
      "Source: 28-pl\n",
      "Content: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)  right or remedy.\n",
      "\n",
      "11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation  in force of the remainder of the term (if any) and this Agreement.\n",
      "\n",
      "11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any  kind between the parties.\n",
      "\n",
      "11.9 No Third-Party Beneficiaries.\n",
      "Source: 30-pl\n",
      "Content: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\n",
      "Source: 4-pl\n",
      "=========\n",
      "FINAL ANSWER: This Agreement is governed by English law.\n",
      "SOURCES: 28-pl\n",
      "\n",
      "QUESTION: What did the president say about Michael Jackson?\n",
      "=========\n",
      "Content: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n",
      "\n",
      "Last year COVID-19 kept us apart. This year we are finally together again. \n",
      "\n",
      "Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n",
      "\n",
      "With a duty to one another to the American people to the Constitution. \n",
      "\n",
      "And with an unwavering resolve that freedom will always triumph over tyranny. \n",
      "\n",
      "Six days ago, Russia‚Äôs Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \n",
      "\n",
      "He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \n",
      "\n",
      "He met the Ukrainian people. \n",
      "\n",
      "From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \n",
      "\n",
      "Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\n",
      "Source: 0-pl\n",
      "Content: And we won‚Äôt stop. \n",
      "\n",
      "We have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \n",
      "\n",
      "Let‚Äôs use this moment to reset. Let‚Äôs stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \n",
      "\n",
      "Let‚Äôs stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \n",
      "\n",
      "We can‚Äôt change how divided we‚Äôve been. But we can change how we move forward‚Äîon COVID-19 and other issues we must face together. \n",
      "\n",
      "I recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \n",
      "\n",
      "They were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \n",
      "\n",
      "Officer Mora was 27 years old. \n",
      "\n",
      "Officer Rivera was 22. \n",
      "\n",
      "Both Dominican Americans who‚Äôd grown up on the same streets they later chose to patrol as police officers. \n",
      "\n",
      "I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\n",
      "Source: 24-pl\n",
      "Content: And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \n",
      "\n",
      "To all Americans, I will be honest with you, as I‚Äôve always promised. A Russian dictator, invading a foreign country, has costs around the world. \n",
      "\n",
      "And I‚Äôm taking robust action to make sure the pain of our sanctions  is targeted at Russia‚Äôs economy. And I will use every tool at our disposal to protect American businesses and consumers. \n",
      "\n",
      "Tonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \n",
      "\n",
      "America will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \n",
      "\n",
      "These steps will help blunt gas prices here at home. And I know the news about what‚Äôs happening can seem alarming. \n",
      "\n",
      "But I want you to know that we are going to be okay.\n",
      "Source: 5-pl\n",
      "Content: More support for patients and families. \n",
      "\n",
      "To get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \n",
      "\n",
      "It‚Äôs based on DARPA‚Äîthe Defense Department project that led to the Internet, GPS, and so much more.  \n",
      "\n",
      "ARPA-H will have a singular purpose‚Äîto drive breakthroughs in cancer, Alzheimer‚Äôs, diabetes, and more. \n",
      "\n",
      "A unity agenda for the nation. \n",
      "\n",
      "We can do this. \n",
      "\n",
      "My fellow Americans‚Äîtonight , we have gathered in a sacred space‚Äîthe citadel of our democracy. \n",
      "\n",
      "In this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \n",
      "\n",
      "We have fought for freedom, expanded liberty, defeated totalitarianism and terror. \n",
      "\n",
      "And built the strongest, freest, and most prosperous nation the world has ever known. \n",
      "\n",
      "Now is the hour. \n",
      "\n",
      "Our moment of responsibility. \n",
      "\n",
      "Our test of resolve and conscience, of history itself. \n",
      "\n",
      "It is in this moment that our character is formed. Our purpose is found. Our future is forged. \n",
      "\n",
      "Well I know this nation.\n",
      "Source: 34-pl\n",
      "=========\n",
      "FINAL ANSWER: The president did not mention Michael Jackson.\n",
      "SOURCES:\n",
      "\n",
      "QUESTION: What does Barbie have to do with Oppenheimer?\n",
      "=========\n",
      "Content: Barbenheimer is an Internet phenomenon that began on social media before the simultaneous theatrical release of two blockbuster films, Barbie and Oppenheimer, on July 21, 2023, in the United States and several other countries. The word is a portmanteau of the films' titles. The contrast of Barbie‚Äîa fantasy comedy by Greta Gerwig about the fashion doll Barbie‚Äîand Oppenheimer‚Äîan epic biographical thriller by Christopher Nolan about physicist J. Robert Oppenheimer, scientific director of the Manhattan Project, which developed the first nuclear weapons during World War II‚Äîprompted a comedic response from Internet users, including memes and merchandise.\n",
      "Source: https://en.wikipedia.org/wiki/Barbenheimer\n",
      "\n",
      "Content: Barbenheimer is an Internet phenomenon that began on social media before the simultaneous theatrical release of two blockbuster films, Barbie and Oppenheimer, on July 21, 2023, in the United States and several other countries. The word is a portmanteau of the films' titles. The contrast of Barbie‚Äîa fantasy comedy by Greta Gerwig about the fashion doll Barbie‚Äîand Oppenheimer‚Äîan epic biographical thriller by Christopher Nolan about physicist J. Robert Oppenheimer, scientific director of the Manhattan Project, which developed the first nuclear weapons during World War II‚Äîprompted a comedic response from Internet users, including memes and merchandise.\n",
      "Source: https://en.wikipedia.org/wiki/Barbenheimer\n",
      "\n",
      "Content: Barbenheimer is an Internet phenomenon that began on social media before the simultaneous theatrical release of two blockbuster films, Barbie and Oppenheimer, on July 21, 2023, in the United States and several other countries. The word is a portmanteau of the films' titles. The contrast of Barbie‚Äîa fantasy comedy by Greta Gerwig about the fashion doll Barbie‚Äîand Oppenheimer‚Äîan epic biographical thriller by Christopher Nolan about physicist J. Robert Oppenheimer, scientific director of the Manhattan Project, which developed the first nuclear weapons during World War II‚Äîprompted a comedic response from Internet users, including memes and merchandise.\n",
      "Source: https://en.wikipedia.org/wiki/Barbenheimer\n",
      "=========\n",
      "FINAL ANSWER:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Barbie and Oppenheimer are two blockbuster films that were released simultaneously on July 21, 2023. The contrast between the two films, Barbie being a fantasy comedy and Oppenheimer being an epic biographical thriller about physicist J. Robert Oppenheimer, led to an Internet phenomenon called Barbenheimer. This phenomenon included memes and merchandise related to the combination of the two films. \\nSOURCES: https://en.wikipedia.org/wiki/Barbenheimer'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What does Barbie have to do with Oppenheimer?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üõ∏ TASK**: Build your own `ConversationalRetrievalChain` using a different data source to index in a vector store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is my own example..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö Data Augmented Generation: Chaining it all together\n",
    "\n",
    "Great! We've learned about:\n",
    "\n",
    "1. **Document Loaders**: Ways to load external data sources into our LLM application.\n",
    "2. **Retrival**: How to use a vector database to retrieve the most relevant datapoints from our external data sources."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßê A note on Evaluation\n",
    "\n",
    "Throughout these exercises, responses from LLMs and chat models have not always been accurate. Evaluating LLMs systems is the wild west. There are some ways to evaluate compontents of a system like A/B testing prompts and examples or using LLMs to evaluate the quality of its responses. \n",
    "\n",
    "To learn more about evaluating LLM applications, check out [this video of Josh Tobin discussing evaluation from LLMs in prod conference](https://www.youtube.com/watch?v=r-HUnht-Gns)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üçâ Conclusions\n",
    "\n",
    "From this tutorial, you should have a sense of how to build LLM applications using LangChain. You've familiarised yourself with:\n",
    "\n",
    "1. **Prompting.** How to make use of a prompt template, the benefits of few-shot prompting, how to format instructions to parse the output of a model.\n",
    "\n",
    "2. **Chains & Agents.** How to write a simple LLMChain, how to build a sequential chain and investigating agents.  \n",
    "\n",
    "3. **Data Augmented Generation.** How to use document loaders to load external data sources; how to create a vector database and store external data as embeddings; how to build a qa chain using a vector database.\n",
    "\n",
    "Happy LangChain-ing! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dap_taltech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:42:20) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10280fdd29a5f790370cdf255c4a66c215f248ca8aa68590910e05113a9680f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
