{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤖 Topic modeling with Latent Dirichlet Allocation 🤖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dap_taltech.utils.data_getters import DataGetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = DataGetter(local=True)\n",
    "data = getter.get_oa_articles(institution=\"TT_p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Topic Modeling with Latent Dirichlet Allocation (LDA) is a widely-used method in text analysis. The foundational assumption in LDA is that documents are composed of a mixture of topics, and these topics can be expressed as distributions of words.\n",
    "\n",
    "The goal of LDA topic modeling is to discover the underlying topics within a corpus and understand how they are expressed within the documents. To this end, LDA uses Bayesian inference techniques to estimate:\n",
    "\n",
    "1. Topic-Document Distribution ($\\theta$): The probability of each topic being expressed in each document.\n",
    "2. Word-Topic Distribution ($\\beta$): The probability of each word being assigned to each topic.\n",
    "\n",
    "These two distributions are inferred based on the words observed in the corpus and their co-occurrence across documents.\n",
    "\n",
    "Dirichlet priors are assigned to these distributions, encapsulating our prior belief that each document contains only a few topics and each topic is represented by a subset of the entire vocabulary.\n",
    "\n",
    "In plate notation, the graphical model of LDA is a three-level generative model that looks like this:\n",
    "\n",
    "![plate](https://scikit-learn.org/stable/_images/lda_model_graph.png)\n",
    "\n",
    "The parameters above can be described as:\n",
    "- $D$ denotes the number of documents in our corpus-\n",
    "- $K$ denotes the number of topics we exogenously chose for our data.\n",
    "- $V$ refers to the vocabulary of words in our corpus.\n",
    "- $N_d$ is the number of words a given document $d$ has.\n",
    "- $w_{d,n}$ refers to each word in document $d$ and indexed in $N_d$ as $n$.\n",
    "- $z_{d,n}$ is the topic assignment for word $n$ in document $d$, which is drawn from a multinomial distribution.\n",
    "- $\\theta_d$ is the topic distribution for document $d$, a probability vector or multinomial distribution over topics.\n",
    "- $\\beta_k$ is the word distribution for topic $k$, a probability vector over words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model assumes the following generative process for a corpus with $D$ documents and $K$ topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For each topic $k \\in K$, draw $\\beta_k \\sim Dirichlet(\\eta)$. This provides a distribution over the words\n",
    "\n",
    "2. For each document $d \\in D$, draw the topic proportions $\\theta_d \\sim Dirichlet(\\alpha)$\n",
    "\n",
    "3. For each word $i$ in document $d$\n",
    "\n",
    "    1. Draw the topic assignment $z_{di} \\sim Multinomial(\\theta_d)$\n",
    "    \n",
    "    2. Draw the observed word $w_{ij} \\sim Multinomial(\\beta_{z_{di}})$\n",
    "    \n",
    "Bayesian inference derives the posterior probability from two elements: a _prior probability_ and a _likelihood function_ derived from a statistical model of the observed data (here, words and documents). A key concern is, however, how to compute the posterior distribution of all these hidden variables given documents. \n",
    "    \n",
    "A stylized posterior distribution reads: \n",
    "$$\n",
    "p(z, \\theta, \\beta \\mid w, \\alpha, \\eta)=\\frac{p(z, \\theta, \\beta, w \\mid \\alpha, \\eta)}{p(w \\mid \\alpha, \\eta)}\n",
    "$$\n",
    "\n",
    "As in many other complex statistical models filled with latent variables, this posterior distribution is in practice intractable (this is due to the coupling between $\\theta$ and $\\beta$ in the summation over latent\n",
    "topics), thus requiring of approximate inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 📂 Unpacking $\\theta$ and $\\beta$\n",
    "$\\theta$ is a matrix where each cell at the intersection of row $d$ and column $j$ represents the probability of document $d$ being associated with topic $j$. As this probabilistic assignment is drawn from a multinomial distribution, $\\theta$ can be seen as a distribution over these distributions.\n",
    "\n",
    "The properties of Dirichlet distributions, being the conjugate priors for multinomial distributions, make them an ideal choice for modeling $\\theta$ and $\\beta$. This means that the prior and posterior distributions of multinomial parameters are both Dirichlet, simplifying the computation of the posterior.\n",
    "\n",
    "#### 🕵 Understanding Dirichlet Distributions\n",
    "A Dirichlet Distribution is the conjugate prior for the probabilities $p_1, ... p_k$, governed by concentration parameters $\\alpha_1, ... \\alpha_k$. The $\\alpha$ values play a vital role in shaping the distribution. A high $\\alpha_i$ increases the likelihood of observing $x_i$, while an $\\alpha_i<1$ pushes $x_i$ towards the extremes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,18))\n",
    "alphas = [[1, 3, 4], [10, 0.2, 0.2], [15,15,15], [1, 1, 1], [0.1, 0.1, 0.1], [0.01, 0.01, 0.01]]\n",
    "for i, tripl in enumerate(alphas):\n",
    "    theta = stats.dirichlet(tripl).rvs(500)\n",
    "    \n",
    "    ax = fig.add_subplot(3, 2, i+1, projection='3d')\n",
    "    plt.title(r'$\\alpha$ = {}'.format(tripl))\n",
    "    ax.scatter(theta[:, 0], theta[:, 1], theta[:, 2])\n",
    "    ax.view_init(azim=30)\n",
    "    ax.set_xlabel(r'$\\theta_1$')\n",
    "    ax.set_ylabel(r'$\\theta_2$')\n",
    "    ax.set_zlabel(r'$\\theta_3$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🔑 Insights on $\\beta$, $\\theta$, and Sparsity in LDA\n",
    "A central aspect of LDA topic modeling lies in the sparse structure induced by modeling $\\beta$ and $\\theta$ as random variables drawn from Dirichlet processes with small $\\alpha$ and $\\eta$ parameters.\n",
    "\n",
    "The low values of $\\alpha$ and $\\eta$ contribute to a tighter clustering of documents to topics via the $\\theta$ matrix and words to topics through the $\\beta$ matrix. This sparsity encourages a model where documents are associated with a small set of prominent topics and each topic is represented by a relatively limited set of distinct words.\n",
    "\n",
    "This structure aligns with our intuitive understanding of how documents and topics relate. It is common for a document to revolve around a few primary themes, rather than a broad range of unrelated topics. Similarly, while a topic can include many words, only a subset of those words are often key to defining the core idea of the topic.\n",
    "\n",
    "This sparsity also serves a penalization function. It discourages models where a document is associated with a large number of topics, or where a topic is represented by an excessively large set of words. This penalty helps the model to maintain a clear and succinct representation of the themes present in the corpus.\n",
    "\n",
    "Therefore, the choice of $\\alpha$ and $\\eta$ in the Dirichlet distributions for $\\theta$ and $\\beta$ plays a pivotal role in shaping the thematic structure and clarity of the resulting LDA topic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple methods of approximating the posterior exist in Bayesian statistics, with **Gibbs Sampling** and **variational inference** being traditionally applied the most. In what follows we mostly focus on the former, but present the latter briefly since in practice it's more easily implemented. For a complete review on variational inference methods, check [this paper](https://arxiv.org/pdf/1601.00670.pdf) by Blei and co-authors. For a non-LDA related introduction to Gibbs sampling, take a look [here](https://drum.lib.umd.edu/bitstream/handle/1903/10058/gsfu.pdf?sequence=3&isAllowed=y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variational inference\n",
    "\n",
    "Variational inference seeks to approximate the intractable posterior with some well-known and well-behaved probability distribution that closely matches the true posterior. Using Kullback-Leibler divergence measures, the algorithm optimizes over a family of distributions and picks the member that best mimics the exact posterior, ie. \n",
    "\n",
    "$$\n",
    "\\gamma^{\\star}, \\phi^{\\star}, \\lambda^{\\star}=\\operatorname{argmin}_{(\\gamma, \\phi, \\lambda)} D(q(\\theta, \\mathbf{z}, \\beta \\mid \\gamma, \\phi, \\lambda) \\| p(\\theta, \\mathbf{z}, \\beta \\mid \\mathcal{D} ; \\alpha, \\eta)\n",
    "$$\n",
    "\n",
    "where $\\gamma, \\phi$ and $\\lambda$ represent variational parameters used to approximate $\\theta, z$ and $\\beta$, respectively. The $D()$ function represents the KL divergence between a member distribution $q$ and the true posterior $p$. This method provides a locally-optimal exact-analytical solution to an approximation of the posterior distribution.\n",
    "\n",
    "#### Gibbs Sampling\n",
    "\n",
    "Gibbs Sampling is an alternative empirical method of approximating the posterior. A Monte Carlo Markov Chain method, the basic idea is to iteratetively compute the posterior of each of the latent variables by sampling from conditional distributions where all other latent variables are fixed and treated as known. Importantly, the variables used to condition are constantly updated on their most recent expected distributions, in the hopes that we gradually inch closer to the posterior joint distribution. \n",
    "\n",
    "In more practical terms, we iterate over words in a document and we estimate the conditional probability distribution of the word's specific topic assignment given all other topic assignments. Mathematically, we will find the conditional probability distribution of a single word topic assignment conditioned on the rest of the model:\n",
    "\n",
    "$$\n",
    "p\\left(z_{d, n}=k \\mid \\vec{z}_{-d, n}, \\vec{w}, \\alpha, \\eta\\right)=\\frac{p\\left(z_{d, n}=k, \\vec{z}_{-d, n} \\mid \\vec{w}, \\alpha, \\eta\\right)}{p\\left(\\vec{z}_{-d, n} \\mid \\vec{w}, \\alpha, \\eta\\right)}\n",
    "$$\n",
    "\n",
    "We won't delve on the math, but note that due to the special structure of the LDA model we are able to integrate out both $\\theta$ and $\\beta$ in the equation above (in jargon, we are able to marginalize the target posterior over $\\beta$ and $\\theta$). This dramatically reduces the space in which we explore in the Gibbs Sampler, which is convenient since it will converge to a stationary posterior at a faster rate. The algorithm for this marginalized posterior is known as the **collapsed Gibbs Sampler**. After integrating out, the conditional probability distribution reads:\n",
    "\n",
    "$$\n",
    "p\\left(z_{d, n}=k \\mid \\vec{z}_{-d, n}, \\vec{w}, \\alpha, \\eta\\right)=\\frac{n_{d, k}+\\alpha_{k}}{\\sum_{i}^{K} \\left(n_{d, i}+\\alpha_{i}\\right)} \\frac{v_{k, w_{d, n}}+\\eta_{w_{d, n}}}{\\sum_{w \\in V} \\left(v_{k, i}+\\eta_{i}\\right)}\n",
    "$$\n",
    "\n",
    "There are two parts to the equation above. First part tells us how much each topic is present in a document, while the second part tells us a topic's affinity towards a word. Since this is a probability distribution, for each word we will get a vector of probabilities. After this probability is computed, we sample a new $z$ assignment for the word. In pseudo-code, this reads:\n",
    "\n",
    "  1. Decrement $n_{d, z_{old}}$ and $v_{w_{d,n}, z_{old}}$\n",
    "  2. Sample $z_{new}=k$ with probability proportional to $p\\left(z_{d, n}=k \\mid \\vec{z}_{-d, n}, \\vec{w}, \\alpha, \\eta\\right)$\n",
    "  3. Increment $n_{d, z_{new}}$ and $v_{w_{d,n}, z_{new}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sampling **$z \\mid w$** with Gibbs sampling, one can recover $\\theta$ and $\\beta$ with\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\hat{\\beta}_{k, w_{n}}=\\frac{n_{k, w_{n}}+\\eta}{n_{K}+V \\eta} \\\\\n",
    "&\\hat{\\theta}_{d k}=\\frac{n_{d k}+\\alpha}{n_{d}+K \\alpha}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which are marginalized versions of the first and second term of the equation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🐣 Gibbs sampler example\n",
    "It may be instructive to first present a quick example of a manually-built topic model where we use Gibbs sampler to sequentially update topic assignments to words. Other parts of this script will refrain from using this code (which is slightly simplified for speed following [this paper](https://www.ics.uci.edu/~asuncion/pubs/KDD_08.pdf)), and instead we will use off-the-shelve modules from well-known Python libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ = 1000\n",
    "\n",
    "# Fetch training data from sklearn\n",
    "data20, _ = fetch_20newsgroups(\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    return_X_y=True,\n",
    ")\n",
    "\n",
    "data_sample = data20[:n_]\n",
    "\n",
    "# Define matrix\n",
    "tf_vect = CountVectorizer(\n",
    "    max_df=0.8, \n",
    "    min_df=2,\n",
    "    max_features=10_000,\n",
    "    stop_words='english'\n",
    ")\n",
    "tf = tf_vect.fit_transform(data_sample)\n",
    "\n",
    "# Get vocabulary\n",
    "voc = tf_vect.vocabulary_\n",
    "\n",
    "idx_voc = tf_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we convert our term frequency matrix to a list of documents where each document is represented as a list of word indices (with frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the document indices\n",
    "docs = []\n",
    "\n",
    "# Loop over each document (represented as a row in the term frequency matrix)\n",
    "for row in tf.toarray():\n",
    "    # Identify the indices of words that appear at least once in the document\n",
    "    words = np.where(row != 0)[0].tolist()\n",
    "    \n",
    "    # Initialize an empty list to store the word indices, accounting for word frequency\n",
    "    word_counts = []\n",
    "    \n",
    "    # Loop over each word index\n",
    "    for word_idx in words:\n",
    "        # For each occurrence of the word in the document, add the word's index to word_counts\n",
    "        for i in range(row[word_idx]):\n",
    "            word_counts.append(word_idx)\n",
    "    \n",
    "    # Append the list of word indices (with frequency) for this document to the overall docs list\n",
    "    docs.append(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize our parameters and hyperparameters for the Gibbs sampler. 'K' represents the number of topics we wish to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of documents\n",
    "D = len(docs)\n",
    "\n",
    "# size of the vocabulary \n",
    "Voc = len(voc)  \n",
    "\n",
    "# number of topics\n",
    "K = 10   \n",
    "\n",
    "# Dirichlet prior on per-document topic distribution\n",
    "alpha = 0.2\n",
    "\n",
    "# Dirichlet prior on per-topic word distribution\n",
    "eta = 1 / K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize count vectors and other necessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_d_n = [[0 for _ in range(len(d))] for d in docs]  # z_i_j # Assignment of topic to word n in document d\n",
    "n_d_k = np.zeros((D, K)) # Count vector in document d for topic k \n",
    "v_k_w = np.zeros((K, Voc)) # Count vector of topic k for term w\n",
    "N_d = np.zeros((D)) # Count of words in document d\n",
    "V_k = np.zeros((K)) # Count of terms in topic k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's track a specific sample to see the effects of MCMCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 1\n",
    "\n",
    "data_sample[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of the Gibbs sampler. We randomly assign each word in each document to one of the 'K' topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from utils import parse_text\n",
    "## Initialize parameters with random counts\n",
    "\n",
    "# m → doc id\n",
    "for d, doc in enumerate(docs):  \n",
    "    # n → id of word inside document\n",
    "    # w → id of the word in the global vocabulary\n",
    "    \n",
    "    for n, w in enumerate(doc):\n",
    "        # Assign a t=0 topic to each word\n",
    "        z_d_n[d][n] = random.randrange(K)\n",
    "        # Retreive it and assign it to the count vectors\n",
    "        z = z_d_n[d][n]\n",
    "        n_d_k[d][z] += 1\n",
    "        N_d[d] += 1\n",
    "        v_k_w[z, w] += 1\n",
    "        V_k[z] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main Gibbs sampling loop. For each iteration, we loop over each word in each document, and resample its topic assignment conditioned on all other words' assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter_ in tqdm(range(10)):\n",
    "    parse_text(docs, data_sample, test, idx_voc, z_d_n)\n",
    "    print('\\n')\n",
    "    for d, doc in enumerate(docs):\n",
    "        for n, w in enumerate(doc):\n",
    "            # Fetch previously-assigned topic for word w_d,n\n",
    "            z = z_d_n[d][n]\n",
    "\n",
    "            # Decrement counts\n",
    "            n_d_k[d][z] -= 1\n",
    "            v_k_w[z, w] -= 1\n",
    "            V_k[z] -= 1\n",
    "\n",
    "            ## Sample a new topic assignment from a multinomial\n",
    "            # How much a document likes a particular topic\n",
    "            p_d_t = (n_d_k[d] + alpha) / (N_d[d] - 1 + K * alpha)\n",
    "            \n",
    "            # How much a topic likes a particular word\n",
    "            p_t_w = (v_k_w[:, w] + eta) / (V_k + Voc * eta)\n",
    "            \n",
    "            p_z = p_d_t * p_t_w\n",
    "            p_z /= np.sum(p_z)\n",
    "            \n",
    "            # Draw from a multinomial pmf the new topic assignment\n",
    "            new_z = np.random.multinomial(1, p_z).argmax()\n",
    "\n",
    "            # Update and increment counts according to new assignment\n",
    "            z_d_n[d][n] = new_z\n",
    "            n_d_k[d][new_z] += 1\n",
    "            v_k_w[new_z, w] += 1\n",
    "            V_k[new_z] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the Gibbs sampling procedure, we can inspect the learned topic-word distribution and document-topic distribution. Here we visualize the document-topic distribution for one document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_d_k[test]/ sum(n_d_k[test]))\n",
    "plt.title(f'Topic distribution $theta_i$ for document {test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the top words in each topic to understand what the topics represent. We do this by looking at the words with the highest probability in each topic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_vocabulary = {v: k for k, v in voc.items()}\n",
    "n_top_words = 15\n",
    "\n",
    "for topic_idx, topic in enumerate(v_k_w):\n",
    "    message = f'Topic #{topic_idx}: '\n",
    "    message += \" \".join([inv_vocabulary[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic modelling implementations using 🍅[tomotopy](https://bab2min.github.io/tomotopy)🍅\n",
    "\n",
    "Let's explore more efficient implementations of topic modelling. There exist plenty of Python libraries that implement specific flavours of topic modelling. In this section, we explore one of the more popular ones, tomotopy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a Corpus object which will contain our text data. We also specify a simple tokenizer and a lambda function to remove stop words, which here we use to remove very rare tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = tp.utils.Corpus(tokenizer=tp.utils.SimpleTokenizer(), stopwords=lambda x: len(x) <= 2)\n",
    "\n",
    "corpus.process(data[\"preprocessed_abstract\"].tolist())\n",
    "\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're looking for unigrams, bigrams and trigrams in our text data. The way this is implemented in tomotopy requires one to first create the tokens (which we did in the cell above) and then extract n_grams from the corpus, which will substitute the constituent unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have added all unigrams, let's also consider bigrams and trigrams\n",
    "cands = corpus.extract_ngrams(min_cf=10, min_df=5, max_len=3, max_cand=2_500, normalized=True)\n",
    "corpus.concat_ngrams(cands)\n",
    "\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Latent Dirichlet Allocation topic modelling 🍅\n",
    "\n",
    "The cell below trains the model. Note that tomotopy implements Gibbs sampling, and thus a Markov Chain Monte Carlo that continuously sample from the model's assumed distributions. We omit a discussion over stopping criteria, instead running the model for long enough to assume some degree of stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make LDA model and train\n",
    "mdl = tp.LDAModel(k=20, min_cf=10, min_df=5, corpus=corpus)\n",
    "mdl.train(0)\n",
    "print(f'Num docs:{len(mdl.docs)}, Vocab size: {len(mdl.used_vocabs)}, Num words: {mdl.num_words}')\n",
    "print(f'Removed top words: {mdl.removed_top_words}')\n",
    "for i in range(0, 2000, 10):\n",
    "    mdl.train(10, workers=0)\n",
    "    print('Iteration: {}\\tLog-likelihood: {}'.format(i, mdl.ll_per_word))\n",
    "\n",
    "# mdl.save('path/to/save/model.lda.bin', True)\n",
    "\n",
    "mdl.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's our model trained! Let's have a look at some of the top words for each topic, where we assumed 20 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(mdl.k):\n",
    "    print(f'Top 10 words of topic #{k}')\n",
    "    print(mdl.get_topic_words(k, top_n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the unsupervised nature of the method, our topics are unlabelled. We can remediate that by resorting to PMI scores for a topic's constituent n-grams. Intuitively, a topic is best labelled using n-grams who appear disproportionately as part of a given topic, relative to how these n-grams appear in other topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract candidates using Pointwise Mutual Information (PMI)\n",
    "extractor = tp.label.PMIExtractor(min_cf=10, min_df=5, max_len=5, max_cand=10000)\n",
    "cands = extractor.extract(mdl)\n",
    "cands[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to select the top label candidates, we use FORelevance. The First-order relevance implementation for topic labeling involves using probabilistic approaches to automatically generate labels for multinomial topic models by minimizing Kullback-Leibler divergence and maximizing mutual information, which we've just computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic labeling using candidates and the First-Order Relevance implementation from Mei, Q., Shen, X., & Zhai, C. (2007)\n",
    "labeler = tp.label.FoRelevance(mdl, cands, min_df=5, smoothing=1e-2, mu=0.25)\n",
    "for k in range(mdl.k):\n",
    "    print(f\"== Topic #{k} ==\")\n",
    "    print(\"Labels:\", ', '.join(label for label, score in labeler.get_topic_labels(k, top_n=5)))\n",
    "    for word, prob in mdl.get_topic_words(k, top_n=5):\n",
    "        print(f'{word:<{16}} {round(prob,4)}', sep='\\t')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect some of the relevant information that results from running the LDA routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_term_dists = np.stack([mdl.get_topic_word_dist(k) for k in range(mdl.k)])\n",
    "doc_topic_dists = np.stack([doc.get_topic_dist() for doc in mdl.docs])\n",
    "doc_topic_dists /= doc_topic_dists.sum(axis=1, keepdims=True)\n",
    "doc_lengths = np.array([len(doc.words) for doc in mdl.docs])\n",
    "vocab = list(mdl.used_vocabs)\n",
    "term_frequency = mdl.used_vocab_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a start, let's look at the document topic distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(doc_topic_dists).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly look at how a given document is made up of token words, which have topic assignments. In this illustrative example, we assign a color to a token given its highest topic probability.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicterm_df = pd.DataFrame(topic_term_dists, columns=vocab)\n",
    "# for each word in corpus[0], find the column in f with corresonding string name, return index with highest value\n",
    "tuples = []\n",
    "for x in corpus[0]:\n",
    "    try:\n",
    "        tuples.append((x, topicterm_df[x].idxmax(), topicterm_df[x].max()/topicterm_df[x].sum()))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# color the words in the text\n",
    "from utils import color_tomotopy_string\n",
    "color_tomotopy_string(corpus[0], tuples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use pyLDAvis to visualise the topics, the relevance of the constituent tokens, and their representation in our corpus of TalTech publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tmplot as tmp\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "prepared_data = pyLDAvis.prepare(\n",
    "    topic_term_dists, \n",
    "    doc_topic_dists, \n",
    "    doc_lengths, \n",
    "    vocab, \n",
    "    term_frequency,\n",
    "    start_index=0, # tomotopy starts topic ids with 0, pyLDAvis with 1\n",
    "    sort_topics=False # IMPORTANT: otherwise the topic_ids between pyLDAvis and tomotopy are not matching!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🛸 **TASK**: How do hyperparameters like the number of topics, alpha, and beta affect the results of LDA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt: Experiment with different hyperparameters using tomotopy's LDA implementation. \n",
    "# Observe how the topics change and discuss the impact of these hyperparameters. \n",
    "# Refer to the tomotopy documentation to understand what each hyperparameter controls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also investigate the evolution of topic shares over time at TalTech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [f\"Topic {x}: \" + ', '.join(label for label, score in labeler.get_topic_labels(x, top_n=5)) for x in range(20)]\n",
    "\n",
    "# generate plot data\n",
    "data_lda = (\n",
    "    data\n",
    "    .copy()\n",
    "    .merge(pd.DataFrame(doc_topic_dists, columns=column_names), left_index=True, right_index=True)\n",
    ")\n",
    "\n",
    "# compute quarterly measures\n",
    "data_lda = (\n",
    "    data_lda\n",
    "    .groupby(pd.Grouper(key='publication_date', freq='Q'))[[col for col in data_lda if \"Topic\" in col]].mean()\n",
    "    .iloc[100:, :]\n",
    "    .reset_index(drop=False)\n",
    "    .rename(columns={\"publication_date\": \"Q-year\"})\n",
    "    .melt(id_vars=\"Q-year\", var_name=\"topic\", value_name=\"proportion\")\n",
    ")\n",
    "\n",
    "# plot\n",
    "alt.Chart(data_lda).mark_bar().encode(\n",
    "    alt.X(\n",
    "        'Q-year:T', \n",
    "        axis=alt.Axis(format='%b-%Y', title='Year', tickCount='year'),\n",
    "        scale=alt.Scale(nice='month')\n",
    "    ),\n",
    "    alt.Y('proportion:Q', title='Topic Proportion', stack='normalize'),\n",
    "    alt.Color('topic:N', title='Topic label', legend=None),\n",
    "    tooltip=['topic', 'proportion']\n",
    ").properties(\n",
    "    width=800,\n",
    "    height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An unanswered question is how we choose the number of topics. There typically isn't free lunch - methods that automatically choose the number of topics will have some other hyperparameter that regulates this and needs your attention. \n",
    "\n",
    "Some traditional alternatives that work across approaches include:\n",
    "\n",
    "1. Perplexity: Perplexity is a measure of how well a probability model predicts a sample. You can train multiple models with different values of K and choose the one with the lowest perplexity on a held-out test set.\n",
    "\n",
    "2. Coherence: Coherence measures the semantic similarity between high-scoring words in a topic. You can train multiple models with different values of K and choose the one with the highest coherence score.\n",
    "\n",
    "3. Human judgment: You can train multiple models with different values of K and manually evaluate the interpretability and coherence of the resulting topics. Choose the value of K that produces topics that are meaningful and easy to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🛸 **TASK**: Implement measures of perplexity ([see](https://bab2min.github.io/tomotopy/v/en/#tomotopy.LDAModel.perplexity) the native implementation)and coherence ([see](https://github.com/bab2min/tomotopy/blob/main/examples/coherence.py) an example from tomotopy's codebase) for your topic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Other traditional topic modelling approaches 🍅\n",
    "- **[Correlated Topic Model](https://bab2min.github.io/tomotopy/v/en/#tomotopy.CTModel)**\n",
    "- **[Dynamic Topic Model](https://bab2min.github.io/tomotopy/v/en/#tomotopy.DTModel)**\n",
    "- **[Hierarchical LDA](https://bab2min.github.io/tomotopy/v/en/#tomotopy.HLDAModel)**\n",
    "- **[Partially Labelled LDA](https://bab2min.github.io/tomotopy/v/en/#tomotopy.PLDAModel)** (beyond 🍅, see [COREX](https://pypi.org/project/corextopic/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [BERTopic](https://maartengr.github.io/BERTopic/index.html) with [Huggingface](https://huggingface.co/) 🤗\n",
    "\n",
    "BERTopic is a Python library for topic modeling and visualization that utilizes the power of transformer models and c-TF-IDF to create a clear topic representation. It uses transformer models like BERT to convert documents into dense vectors. Then, by leveraging a class-based variant of TF-IDF, it enhances the performance of clustering algorithms like UMAP (Uniform Manifold Approximation and Projection) and HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) to produce more meaningful and interpretable topics.\n",
    "\n",
    "In terms of functionality, BERTopic allows for effective topic reduction and representation. It allows users to select the number of topics to keep after topic modeling has been performed, providing an easy way to decrease or increase the granularity of topics. The visualization of topics and their sizes, as well as the extraction of most representative documents for a given topic, makes it an interactive and flexible tool. \n",
    "\n",
    "Furthermore, BERTopic supports multiple languages and provides an easy interface to use any transformer model available in Hugging Face's model hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![plate](https://maartengr.github.io/BERTopic/algorithm/modularity.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick example\n",
    "\n",
    "The immediate benefit to leveraging Hugging Face's large library is that one can seamlessly push and pull trained topic models to and from the [HF Hub](https://huggingface.co/docs/hub/index). Let's pick a small embedding model, [paraphrase-albert-small-v2](https://huggingface.co/sentence-transformers/paraphrase-albert-small-v2). For more information on the integration between HF and BERTopic, read the recent [blogpost](https://huggingface.co/blog/bertopic) about it.\n",
    "\n",
    "Note you may have some problems running BERTopic, given incompatibilities with old numpy versions. If that's your case, try to upgrade to a newer version of numpy than we run in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    language=\"english\",\n",
    "    n_gram_range=(1, 3),\n",
    "    min_topic_size=50,\n",
    "    nr_topics=\"auto\",\n",
    "    seed_topic_list=None,\n",
    "    embedding_model=\"paraphrase-albert-small-v2\",\n",
    "    umap_model=None\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(data.preprocessed_abstract)\n",
    "# topic_model.push_to_hf_hub('dampudia/taltech_articles')\n",
    "# topic_model = BERTopic.load(\"davanstrien/transformers_issues_topics\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of attributes that you can access after having trained your BERTopic model or loading one from the Hub:\n",
    "\n",
    "\n",
    "| Attribute | Description |\n",
    "|------------------------|---------------------------------------------------------------------------------------------|\n",
    "| topics_               | The topics that are generated for each document after training or updating the topic model. |\n",
    "| probabilities_ | The probabilities that are generated for each document if HDBSCAN is used. |\n",
    "| topic_sizes_           | The size of each topic                                                                      |\n",
    "| topic_mapper_          | A class for tracking topics and their mappings anytime they are merged/reduced.             |\n",
    "| topic_representations_ | The top *n* terms per topic and their respective c-TF-IDF values.                             |\n",
    "| c_tf_idf_              | The topic-term matrix as calculated through c-TF-IDF.                                       |\n",
    "| topic_labels_          | The default labels for each topic.                                                          |\n",
    "| custom_labels_         | Custom labels for each topic as generated through `.set_topic_labels`.                                                               |\n",
    "| topic_embeddings_      | The embeddings for each topic if `embedding_model` was used.                                                              |\n",
    "| representative_docs_   | The representative documents for each topic if HDBSCAN is used.                                                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_freq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic(0)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERTopic has visualisation tools that are similar to those of LDAvis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also reduce the number of topics by hierarchically clustering them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.reduce_topics(data.preprocessed_abstract, nr_topics=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the fact that tokens are now embeddings, we can compute similarity of any entity to topic representation of its neighboring tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_topics, similarity = topic_model.find_topics(\"gpu\", top_n=5)\n",
    "print(similar_topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dap_taltech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
